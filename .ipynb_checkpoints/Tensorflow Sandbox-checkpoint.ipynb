{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NatchaS/anaconda/envs/tensorflow/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_PIX: 784\n",
      "IMG_SIZE: 28\n",
      "TOTAL_SIZE: 70000\n",
      "\n",
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import sklearn.datasets as sk_data\n",
    "import math\n",
    "import time\n",
    "\n",
    "mnist = fetch_mldata('MNIST original', data_home=sk_data.get_data_home())\n",
    "\n",
    "IMG_PIX = mnist.data.shape[1]\n",
    "IMG_SIZE = int(math.sqrt(IMG_PIX))\n",
    "TRAINING_SIZE = 60000\n",
    "TEST_SIZE = 10000\n",
    "TOTAL_SIZE = mnist.data.shape[0]\n",
    "EPOCH = 50\n",
    "BATCH_SIZE = 100\n",
    "WINDOW_SIZE = 10000/BATCH_SIZE\n",
    "\n",
    "print \"IMG_PIX: %s\\nIMG_SIZE: %s\\nTOTAL_SIZE: %s\\n\"%(IMG_PIX, IMG_SIZE, TOTAL_SIZE)\n",
    "\n",
    "def vectorize_y(Y):\n",
    "    v_y = np.zeros((Y.shape[0], max(set(Y.flatten()))+1))\n",
    "    print v_y.shape\n",
    "    for i in range(len(Y)):\n",
    "        v_y[i][Y[i]] = 1.0\n",
    "    return v_y\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    Y = np.array([Y.flatten()]).T\n",
    "    A = np.concatenate((X, Y),axis=1)\n",
    "    np.random.shuffle(A)\n",
    "    X_prime, Y_prime = np.hsplit(A, [-1])\n",
    "#     if (set(Y_prime.flatten())!=set([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0])):\n",
    "#         return shuffle(X,Y)\n",
    "    Y_prime = Y_prime.reshape(Y_prime.shape[0],)\n",
    "    return X_prime, Y_prime\n",
    "\n",
    "def prepare_data(_mnist):\n",
    "    _mnist.data = _mnist.data.astype(np.float32)\n",
    "    _mnist.data = np.multiply(_mnist.data, 1.0 / 255.0)\n",
    "    _mnist.target = _mnist.target.astype(np.uint8)\n",
    "    train_xx, test_xx = np.vsplit(_mnist.data, [TRAINING_SIZE])\n",
    "    train_yy, test_yy = np.array_split(_mnist.target, [TRAINING_SIZE])\n",
    "    return train_xx, test_xx, train_yy, test_yy\n",
    "\n",
    "print type(mnist.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training => (60000, 784) (60000,)\n",
      "testing => (10000, 784) (10000,)\n",
      "<module 'ffbn' from 'ffbn.py'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NatchaS/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:41: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "(STEP 0) loss= 2.30439 acc= 0.11 accum_acc= 0.0011 t= 0.404375076294 sec\n",
      "(STEP 10) loss= 2.07538 acc= 0.41 accum_acc= 0.0351 t= 0.46453499794 sec\n",
      "(STEP 20) loss= 1.88829 acc= 0.67 accum_acc= 0.0905 t= 0.524702072144 sec\n",
      "(STEP 30) loss= 1.83325 acc= 0.65 accum_acc= 0.1488 t= 0.585016965866 sec\n",
      "(STEP 40) loss= 1.78937 acc= 0.67 accum_acc= 0.2155 t= 0.678068161011 sec\n",
      "(STEP 50) loss= 1.77945 acc= 0.72 accum_acc= 0.2856 t= 0.73885512352 sec\n",
      "(STEP 60) loss= 1.78071 acc= 0.7 accum_acc= 0.3602 t= 0.799093008041 sec\n",
      "(STEP 70) loss= 1.61913 acc= 0.87 accum_acc= 0.4428 t= 0.859883069992 sec\n",
      "(STEP 80) loss= 1.669 acc= 0.82 accum_acc= 0.5258 t= 0.953653097153 sec\n",
      "(STEP 90) loss= 1.66814 acc= 0.8 accum_acc= 0.6054 t= 1.02161502838 sec\n",
      "(STEP 100) loss= 1.64964 acc= 0.85 accum_acc= 0.6851 t= 1.17847204208 sec\n",
      "(STEP 110) loss= 1.63145 acc= 0.86 accum_acc= 0.7348 t= 1.31749200821 sec\n",
      "(STEP 120) loss= 1.67346 acc= 0.78 accum_acc= 0.7669 t= 1.47032403946 sec\n",
      "(STEP 130) loss= 1.62876 acc= 0.84 accum_acc= 0.7949 t= 1.6023671627 sec\n",
      "(STEP 140) loss= 1.56375 acc= 0.94 accum_acc= 0.8182 t= 1.67174315453 sec\n",
      "(STEP 150) loss= 1.63525 acc= 0.86 accum_acc= 0.8357 t= 1.81643009186 sec\n",
      "(STEP 160) loss= 1.58353 acc= 0.89 accum_acc= 0.8523 t= 1.95808601379 sec\n",
      "(STEP 170) loss= 1.58435 acc= 0.91 accum_acc= 0.8608 t= 2.0239751339 sec\n",
      "(STEP 180) loss= 1.58852 acc= 0.88 accum_acc= 0.8674 t= 2.08456897736 sec\n",
      "(STEP 190) loss= 1.58349 acc= 0.91 accum_acc= 0.8769 t= 2.15432500839 sec\n",
      "(STEP 200) loss= 1.56282 acc= 0.91 accum_acc= 0.8862 t= 2.26006007195 sec\n",
      "(STEP 210) loss= 1.56679 acc= 0.9 accum_acc= 0.8938 t= 2.32256197929 sec\n",
      "(STEP 220) loss= 1.59814 acc= 0.86 accum_acc= 0.8952 t= 2.38251495361 sec\n",
      "(STEP 230) loss= 1.57869 acc= 0.88 accum_acc= 0.9011 t= 2.45182299614 sec\n",
      "(STEP 240) loss= 1.53537 acc= 0.92 accum_acc= 0.9025 t= 2.55409908295 sec\n",
      "(STEP 250) loss= 1.59449 acc= 0.86 accum_acc= 0.9053 t= 2.63869404793 sec\n",
      "(STEP 260) loss= 1.54572 acc= 0.94 accum_acc= 0.9063 t= 2.73474812508 sec\n",
      "(STEP 270) loss= 1.55869 acc= 0.91 accum_acc= 0.9063 t= 2.9195330143 sec\n",
      "(STEP 280) loss= 1.60731 acc= 0.85 accum_acc= 0.9075 t= 3.00599002838 sec\n",
      "(STEP 290) loss= 1.53953 acc= 0.94 accum_acc= 0.91 t= 3.07088708878 sec\n",
      "(STEP 300) loss= 1.55982 acc= 0.9 accum_acc= 0.9109 t= 3.14258813858 sec\n",
      "(STEP 310) loss= 1.55411 acc= 0.91 accum_acc= 0.912 t= 3.22931814194 sec\n",
      "(STEP 320) loss= 1.52436 acc= 0.95 accum_acc= 0.9156 t= 3.30199694633 sec\n",
      "(STEP 330) loss= 1.5857 acc= 0.89 accum_acc= 0.9161 t= 3.37353110313 sec\n",
      "(STEP 340) loss= 1.51667 acc= 0.95 accum_acc= 0.9146 t= 3.50288200378 sec\n",
      "(STEP 350) loss= 1.54461 acc= 0.93 accum_acc= 0.9165 t= 3.5714609623 sec\n",
      "(STEP 360) loss= 1.51327 acc= 0.96 accum_acc= 0.9157 t= 3.68079710007 sec\n",
      "(STEP 370) loss= 1.50106 acc= 0.97 accum_acc= 0.9169 t= 3.74187397957 sec\n",
      "(STEP 380) loss= 1.55104 acc= 0.92 accum_acc= 0.919 t= 3.80149316788 sec\n",
      "(STEP 390) loss= 1.53652 acc= 0.92 accum_acc= 0.9168 t= 3.86487412453 sec\n",
      "(STEP 400) loss= 1.54534 acc= 0.92 accum_acc= 0.9159 t= 3.96856403351 sec\n",
      "(STEP 410) loss= 1.54651 acc= 0.93 accum_acc= 0.9167 t= 4.05396509171 sec\n",
      "(STEP 420) loss= 1.54927 acc= 0.92 accum_acc= 0.9173 t= 4.23274803162 sec\n",
      "(STEP 430) loss= 1.51127 acc= 0.96 accum_acc= 0.9177 t= 4.32264494896 sec\n",
      "(STEP 440) loss= 1.57258 acc= 0.89 accum_acc= 0.9201 t= 4.38319206238 sec\n",
      "(STEP 450) loss= 1.51335 acc= 0.96 accum_acc= 0.9205 t= 4.4540951252 sec\n",
      "(STEP 460) loss= 1.57095 acc= 0.9 accum_acc= 0.9213 t= 4.55460500717 sec\n",
      "(STEP 470) loss= 1.55535 acc= 0.92 accum_acc= 0.9216 t= 4.61629295349 sec\n",
      "(STEP 480) loss= 1.53203 acc= 0.93 accum_acc= 0.9217 t= 4.73200011253 sec\n",
      "(STEP 490) loss= 1.5737 acc= 0.89 accum_acc= 0.9239 t= 4.85488510132 sec\n",
      "(STEP 500) loss= 1.55847 acc= 0.9 accum_acc= 0.927 t= 5.0195171833 sec\n",
      "(STEP 510) loss= 1.52787 acc= 0.95 accum_acc= 0.926 t= 5.08697199821 sec\n",
      "(STEP 520) loss= 1.54661 acc= 0.91 accum_acc= 0.9257 t= 5.17223596573 sec\n",
      "(STEP 530) loss= 1.51132 acc= 0.95 accum_acc= 0.9246 t= 5.23483610153 sec\n",
      "(STEP 540) loss= 1.55883 acc= 0.9 accum_acc= 0.9236 t= 5.32406401634 sec\n",
      "(STEP 550) loss= 1.50303 acc= 0.97 accum_acc= 0.9233 t= 5.38426613808 sec\n",
      "(STEP 560) loss= 1.50379 acc= 0.97 accum_acc= 0.9264 t= 5.45128417015 sec\n",
      "(STEP 570) loss= 1.5215 acc= 0.95 accum_acc= 0.9286 t= 5.64475417137 sec\n",
      "(STEP 580) loss= 1.54669 acc= 0.91 accum_acc= 0.9285 t= 5.79074811935 sec\n",
      "(STEP 590) loss= 1.49958 acc= 0.97 accum_acc= 0.93 t= 5.85633015633 sec\n",
      "(STEP 599) loss= 1.5399 acc= 0.93 accum_acc= 0.9298 t= 5.99471497536 sec\n",
      "EPOCH: 2\n",
      "(STEP 600) loss= 1.56423 acc= 0.9 accum_acc= 0.9298 t= 6.53724908829 sec\n",
      "(STEP 610) loss= 1.53201 acc= 0.93 accum_acc= 0.9309 t= 6.60088014603 sec\n",
      "(STEP 620) loss= 1.49629 acc= 0.96 accum_acc= 0.932 t= 6.68141007423 sec\n",
      "(STEP 630) loss= 1.50958 acc= 0.96 accum_acc= 0.9354 t= 6.74871897697 sec\n",
      "(STEP 640) loss= 1.53446 acc= 0.93 accum_acc= 0.9367 t= 6.85685396194 sec\n",
      "(STEP 650) loss= 1.51098 acc= 0.95 accum_acc= 0.9389 t= 6.92894697189 sec\n",
      "(STEP 660) loss= 1.48862 acc= 0.99 accum_acc= 0.9389 t= 7.02162313461 sec\n",
      "(STEP 670) loss= 1.49173 acc= 0.98 accum_acc= 0.9377 t= 7.08146715164 sec\n",
      "(STEP 680) loss= 1.50985 acc= 0.96 accum_acc= 0.9394 t= 7.14469099045 sec\n",
      "(STEP 690) loss= 1.49895 acc= 0.98 accum_acc= 0.941 t= 7.24686813354 sec\n",
      "(STEP 700) loss= 1.52439 acc= 0.95 accum_acc= 0.9402 t= 7.38567399979 sec\n",
      "(STEP 710) loss= 1.51889 acc= 0.95 accum_acc= 0.9422 t= 7.50430202484 sec\n",
      "(STEP 720) loss= 1.525 acc= 0.95 accum_acc= 0.9428 t= 7.58483099937 sec\n",
      "(STEP 730) loss= 1.53006 acc= 0.93 accum_acc= 0.9418 t= 7.64895701408 sec\n",
      "(STEP 740) loss= 1.55087 acc= 0.93 accum_acc= 0.9425 t= 7.72578811646 sec\n",
      "(STEP 750) loss= 1.50072 acc= 0.96 accum_acc= 0.9423 t= 7.83316898346 sec\n",
      "(STEP 760) loss= 1.53374 acc= 0.92 accum_acc= 0.9415 t= 7.90706300735 sec\n",
      "(STEP 770) loss= 1.49424 acc= 0.98 accum_acc= 0.942 t= 8.05096006393 sec\n",
      "(STEP 780) loss= 1.55612 acc= 0.91 accum_acc= 0.9431 t= 8.25699710846 sec\n",
      "(STEP 790) loss= 1.49321 acc= 0.97 accum_acc= 0.9431 t= 8.35966205597 sec\n",
      "(STEP 800) loss= 1.51677 acc= 0.95 accum_acc= 0.9456 t= 8.43445706367 sec\n",
      "(STEP 810) loss= 1.5228 acc= 0.94 accum_acc= 0.9453 t= 8.54822802544 sec\n",
      "(STEP 820) loss= 1.54042 acc= 0.92 accum_acc= 0.945 t= 8.70430803299 sec\n",
      "(STEP 830) loss= 1.48256 acc= 0.98 accum_acc= 0.9461 t= 8.88163709641 sec\n",
      "(STEP 840) loss= 1.54835 acc= 0.91 accum_acc= 0.9475 t= 9.02312612534 sec\n",
      "(STEP 850) loss= 1.53686 acc= 0.93 accum_acc= 0.9472 t= 9.22808814049 sec\n",
      "(STEP 860) loss= 1.48215 acc= 0.98 accum_acc= 0.9486 t= 9.34418606758 sec\n",
      "(STEP 870) loss= 1.49362 acc= 0.98 accum_acc= 0.9486 t= 9.46863603592 sec\n",
      "(STEP 880) loss= 1.49219 acc= 0.97 accum_acc= 0.9475 t= 9.55248212814 sec\n",
      "(STEP 890) loss= 1.5683 acc= 0.89 accum_acc= 0.9461 t= 9.62267017365 sec\n",
      "(STEP 900) loss= 1.5325 acc= 0.93 accum_acc= 0.9453 t= 9.72456598282 sec\n",
      "(STEP 910) loss= 1.50777 acc= 0.97 accum_acc= 0.945 t= 9.80553507805 sec\n",
      "(STEP 920) loss= 1.51838 acc= 0.94 accum_acc= 0.945 t= 9.86465501785 sec\n",
      "(STEP 930) loss= 1.53789 acc= 0.93 accum_acc= 0.9452 t= 9.94447016716 sec\n",
      "(STEP 940) loss= 1.51974 acc= 0.95 accum_acc= 0.9451 t= 10.0453190804 sec\n",
      "(STEP 950) loss= 1.52866 acc= 0.94 accum_acc= 0.9455 t= 10.1347119808 sec\n",
      "(STEP 960) loss= 1.49703 acc= 0.97 accum_acc= 0.945 t= 10.2415010929 sec\n",
      "(STEP 970) loss= 1.50886 acc= 0.95 accum_acc= 0.9457 t= 10.322633028 sec\n",
      "(STEP 980) loss= 1.54733 acc= 0.91 accum_acc= 0.9443 t= 10.3861019611 sec\n",
      "(STEP 990) loss= 1.48953 acc= 0.98 accum_acc= 0.9459 t= 10.4614601135 sec\n",
      "(STEP 1000) loss= 1.53016 acc= 0.93 accum_acc= 0.9476 t= 10.5271401405 sec\n",
      "(STEP 1010) loss= 1.52807 acc= 0.93 accum_acc= 0.9474 t= 10.5854990482 sec\n",
      "(STEP 1020) loss= 1.51207 acc= 0.96 accum_acc= 0.9483 t= 10.6549520493 sec\n",
      "(STEP 1030) loss= 1.49531 acc= 0.98 accum_acc= 0.9485 t= 10.7441151142 sec\n",
      "(STEP 1040) loss= 1.52543 acc= 0.92 accum_acc= 0.9486 t= 10.8138389587 sec\n",
      "(STEP 1050) loss= 1.52111 acc= 0.95 accum_acc= 0.9495 t= 10.88007617 sec\n",
      "(STEP 1060) loss= 1.52458 acc= 0.95 accum_acc= 0.9481 t= 10.9496719837 sec\n",
      "(STEP 1070) loss= 1.54033 acc= 0.93 accum_acc= 0.9476 t= 11.0155379772 sec\n",
      "(STEP 1080) loss= 1.52713 acc= 0.93 accum_acc= 0.9488 t= 11.0812041759 sec\n",
      "(STEP 1090) loss= 1.4956 acc= 0.97 accum_acc= 0.9479 t= 11.1478841305 sec\n",
      "(STEP 1100) loss= 1.57354 acc= 0.89 accum_acc= 0.9467 t= 11.2183470726 sec\n",
      "(STEP 1110) loss= 1.5248 acc= 0.94 accum_acc= 0.946 t= 11.2906110287 sec\n",
      "(STEP 1120) loss= 1.50866 acc= 0.96 accum_acc= 0.9465 t= 11.4071190357 sec\n",
      "(STEP 1130) loss= 1.4909 acc= 0.98 accum_acc= 0.947 t= 11.547950983 sec\n",
      "(STEP 1140) loss= 1.4909 acc= 0.97 accum_acc= 0.9479 t= 11.6266610622 sec\n",
      "(STEP 1150) loss= 1.49196 acc= 0.97 accum_acc= 0.9473 t= 11.7131810188 sec\n",
      "(STEP 1160) loss= 1.53967 acc= 0.91 accum_acc= 0.9495 t= 11.7906939983 sec\n",
      "(STEP 1170) loss= 1.49568 acc= 0.97 accum_acc= 0.9504 t= 11.8626990318 sec\n",
      "(STEP 1180) loss= 1.5161 acc= 0.96 accum_acc= 0.9512 t= 11.9267070293 sec\n",
      "(STEP 1190) loss= 1.51599 acc= 0.95 accum_acc= 0.9517 t= 11.9912161827 sec\n",
      "(STEP 1199) loss= 1.48488 acc= 0.98 accum_acc= 0.952 t= 12.0565931797 sec\n",
      "EPOCH: 3\n",
      "(STEP 1200) loss= 1.52527 acc= 0.94 accum_acc= 0.9525 t= 12.465389967 sec\n",
      "(STEP 1210) loss= 1.53579 acc= 0.92 accum_acc= 0.9537 t= 12.5694561005 sec\n",
      "(STEP 1220) loss= 1.50629 acc= 0.96 accum_acc= 0.9549 t= 12.6576070786 sec\n",
      "(STEP 1230) loss= 1.51204 acc= 0.95 accum_acc= 0.9538 t= 12.7358241081 sec\n",
      "(STEP 1240) loss= 1.50577 acc= 0.96 accum_acc= 0.9527 t= 12.8342831135 sec\n",
      "(STEP 1250) loss= 1.48623 acc= 0.98 accum_acc= 0.9546 t= 12.9760630131 sec\n",
      "(STEP 1260) loss= 1.50694 acc= 0.95 accum_acc= 0.9543 t= 13.1029500961 sec\n",
      "(STEP 1270) loss= 1.50253 acc= 0.95 accum_acc= 0.955 t= 13.2374711037 sec\n",
      "(STEP 1280) loss= 1.51873 acc= 0.95 accum_acc= 0.9553 t= 13.349157095 sec\n",
      "(STEP 1290) loss= 1.51903 acc= 0.95 accum_acc= 0.9561 t= 13.4767570496 sec\n",
      "(STEP 1300) loss= 1.49571 acc= 0.97 accum_acc= 0.9568 t= 13.5439360142 sec\n",
      "(STEP 1310) loss= 1.51912 acc= 0.95 accum_acc= 0.9583 t= 13.6030271053 sec\n",
      "(STEP 1320) loss= 1.51216 acc= 0.95 accum_acc= 0.9586 t= 13.6671459675 sec\n",
      "(STEP 1330) loss= 1.49215 acc= 0.97 accum_acc= 0.959 t= 13.7973020077 sec\n",
      "(STEP 1340) loss= 1.47222 acc= 0.99 accum_acc= 0.9598 t= 13.9034070969 sec\n",
      "(STEP 1350) loss= 1.49599 acc= 0.96 accum_acc= 0.9595 t= 13.969274044 sec\n",
      "(STEP 1360) loss= 1.50386 acc= 0.96 accum_acc= 0.9588 t= 14.0454740524 sec\n",
      "(STEP 1370) loss= 1.50363 acc= 0.97 accum_acc= 0.9595 t= 14.1369559765 sec\n",
      "(STEP 1380) loss= 1.50578 acc= 0.96 accum_acc= 0.9602 t= 14.265280962 sec\n",
      "(STEP 1390) loss= 1.52303 acc= 0.95 accum_acc= 0.9611 t= 14.3546850681 sec\n",
      "(STEP 1400) loss= 1.47919 acc= 0.99 accum_acc= 0.9623 t= 14.4191770554 sec\n",
      "(STEP 1410) loss= 1.51821 acc= 0.95 accum_acc= 0.9606 t= 14.5428090096 sec\n",
      "(STEP 1420) loss= 1.50731 acc= 0.96 accum_acc= 0.9589 t= 14.6511871815 sec\n",
      "(STEP 1430) loss= 1.50427 acc= 0.96 accum_acc= 0.9593 t= 14.763657093 sec\n",
      "(STEP 1440) loss= 1.52287 acc= 0.94 accum_acc= 0.9605 t= 14.8533530235 sec\n",
      "(STEP 1450) loss= 1.50032 acc= 0.97 accum_acc= 0.9599 t= 14.9831120968 sec\n",
      "(STEP 1460) loss= 1.50737 acc= 0.95 accum_acc= 0.9596 t= 15.1031889915 sec\n",
      "(STEP 1470) loss= 1.52359 acc= 0.94 accum_acc= 0.9588 t= 15.1811449528 sec\n",
      "(STEP 1480) loss= 1.50882 acc= 0.96 accum_acc= 0.9586 t= 15.2552690506 sec\n",
      "(STEP 1490) loss= 1.51367 acc= 0.94 accum_acc= 0.9582 t= 15.3774130344 sec\n",
      "(STEP 1500) loss= 1.54523 acc= 0.92 accum_acc= 0.9572 t= 15.4944369793 sec\n",
      "(STEP 1510) loss= 1.49768 acc= 0.97 accum_acc= 0.9568 t= 15.574860096 sec\n",
      "(STEP 1520) loss= 1.50169 acc= 0.96 accum_acc= 0.9577 t= 15.6355271339 sec\n",
      "(STEP 1530) loss= 1.48793 acc= 0.98 accum_acc= 0.9574 t= 15.7035429478 sec\n",
      "(STEP 1540) loss= 1.50381 acc= 0.96 accum_acc= 0.9568 t= 15.7804679871 sec\n",
      "(STEP 1550) loss= 1.4835 acc= 0.98 accum_acc= 0.9577 t= 15.9097540379 sec\n",
      "(STEP 1560) loss= 1.49492 acc= 0.97 accum_acc= 0.9591 t= 16.0287070274 sec\n",
      "(STEP 1570) loss= 1.48994 acc= 0.98 accum_acc= 0.9604 t= 16.0907549858 sec\n",
      "(STEP 1580) loss= 1.52336 acc= 0.94 accum_acc= 0.9584 t= 16.1684861183 sec\n",
      "(STEP 1590) loss= 1.50862 acc= 0.95 accum_acc= 0.9591 t= 16.242002964 sec\n",
      "(STEP 1600) loss= 1.49097 acc= 0.97 accum_acc= 0.9585 t= 16.3217051029 sec\n",
      "(STEP 1610) loss= 1.48172 acc= 0.98 accum_acc= 0.9607 t= 16.4362020493 sec\n",
      "(STEP 1620) loss= 1.48692 acc= 0.98 accum_acc= 0.9608 t= 16.5370681286 sec\n",
      "(STEP 1630) loss= 1.48583 acc= 0.98 accum_acc= 0.9616 t= 16.5996119976 sec\n",
      "(STEP 1640) loss= 1.49258 acc= 0.98 accum_acc= 0.9621 t= 16.6787190437 sec\n",
      "(STEP 1650) loss= 1.51262 acc= 0.95 accum_acc= 0.9605 t= 16.7530121803 sec\n",
      "(STEP 1660) loss= 1.48974 acc= 0.98 accum_acc= 0.9606 t= 16.8244431019 sec\n",
      "(STEP 1670) loss= 1.5132 acc= 0.95 accum_acc= 0.9591 t= 16.8996591568 sec\n",
      "(STEP 1680) loss= 1.51907 acc= 0.95 accum_acc= 0.9614 t= 16.9883601665 sec\n",
      "(STEP 1690) loss= 1.57444 acc= 0.89 accum_acc= 0.9593 t= 17.105052948 sec\n",
      "(STEP 1700) loss= 1.49499 acc= 0.97 accum_acc= 0.9608 t= 17.2076330185 sec\n",
      "(STEP 1710) loss= 1.49442 acc= 0.97 accum_acc= 0.9598 t= 17.3029539585 sec\n",
      "(STEP 1720) loss= 1.51345 acc= 0.96 accum_acc= 0.9595 t= 17.4351861477 sec\n",
      "(STEP 1730) loss= 1.54808 acc= 0.92 accum_acc= 0.9578 t= 17.5330121517 sec\n",
      "(STEP 1740) loss= 1.51702 acc= 0.95 accum_acc= 0.957 t= 17.6134080887 sec\n",
      "(STEP 1750) loss= 1.48281 acc= 0.98 accum_acc= 0.9584 t= 17.6854431629 sec\n",
      "(STEP 1760) loss= 1.49901 acc= 0.96 accum_acc= 0.9589 t= 17.7506730556 sec\n",
      "(STEP 1770) loss= 1.50284 acc= 0.96 accum_acc= 0.96 t= 17.8113911152 sec\n",
      "(STEP 1780) loss= 1.50007 acc= 0.96 accum_acc= 0.9613 t= 17.8787600994 sec\n",
      "(STEP 1790) loss= 1.53348 acc= 0.94 accum_acc= 0.9636 t= 17.9960391521 sec\n",
      "(STEP 1799) loss= 1.49569 acc= 0.97 accum_acc= 0.9634 t= 18.1354320049 sec\n",
      "EPOCH: 4\n",
      "(STEP 1800) loss= 1.49081 acc= 0.97 accum_acc= 0.9634 t= 18.5526111126 sec\n",
      "(STEP 1810) loss= 1.49953 acc= 0.96 accum_acc= 0.9654 t= 18.6604671478 sec\n",
      "(STEP 1820) loss= 1.5217 acc= 0.94 accum_acc= 0.9649 t= 18.7242729664 sec\n",
      "(STEP 1830) loss= 1.49089 acc= 0.99 accum_acc= 0.9662 t= 18.7941241264 sec\n",
      "(STEP 1840) loss= 1.49174 acc= 0.97 accum_acc= 0.9677 t= 18.8600549698 sec\n",
      "(STEP 1850) loss= 1.49795 acc= 0.97 accum_acc= 0.9679 t= 18.920372963 sec\n",
      "(STEP 1860) loss= 1.49599 acc= 0.96 accum_acc= 0.966 t= 18.9844629765 sec\n",
      "(STEP 1870) loss= 1.47496 acc= 0.98 accum_acc= 0.9656 t= 19.0730140209 sec\n",
      "(STEP 1880) loss= 1.47601 acc= 0.99 accum_acc= 0.9645 t= 19.1691260338 sec\n",
      "(STEP 1890) loss= 1.50921 acc= 0.96 accum_acc= 0.9643 t= 19.2750270367 sec\n",
      "(STEP 1900) loss= 1.49811 acc= 0.96 accum_acc= 0.9644 t= 19.3565750122 sec\n",
      "(STEP 1910) loss= 1.4974 acc= 0.96 accum_acc= 0.9628 t= 19.4249830246 sec\n",
      "(STEP 1920) loss= 1.46192 acc= 1.0 accum_acc= 0.9635 t= 19.5759439468 sec\n",
      "(STEP 1930) loss= 1.4932 acc= 0.96 accum_acc= 0.9636 t= 19.6858241558 sec\n",
      "(STEP 1940) loss= 1.51772 acc= 0.95 accum_acc= 0.963 t= 19.7530231476 sec\n",
      "(STEP 1950) loss= 1.51344 acc= 0.95 accum_acc= 0.962 t= 19.8291480541 sec\n",
      "(STEP 1960) loss= 1.49319 acc= 0.97 accum_acc= 0.9649 t= 19.8922491074 sec\n",
      "(STEP 1970) loss= 1.49294 acc= 0.99 accum_acc= 0.9659 t= 20.0439929962 sec\n",
      "(STEP 1980) loss= 1.50451 acc= 0.96 accum_acc= 0.9658 t= 20.1897749901 sec\n",
      "(STEP 1990) loss= 1.51163 acc= 0.96 accum_acc= 0.9661 t= 20.2617201805 sec\n",
      "(STEP 2000) loss= 1.51409 acc= 0.95 accum_acc= 0.9658 t= 20.3536300659 sec\n",
      "(STEP 2010) loss= 1.49689 acc= 0.96 accum_acc= 0.9663 t= 20.4136719704 sec\n",
      "(STEP 2020) loss= 1.47297 acc= 0.99 accum_acc= 0.9668 t= 20.4793071747 sec\n",
      "(STEP 2030) loss= 1.51079 acc= 0.96 accum_acc= 0.9676 t= 20.5481760502 sec\n",
      "(STEP 2040) loss= 1.49545 acc= 0.97 accum_acc= 0.9679 t= 20.6151900291 sec\n",
      "(STEP 2050) loss= 1.49305 acc= 0.97 accum_acc= 0.9685 t= 20.7238490582 sec\n",
      "(STEP 2060) loss= 1.49028 acc= 0.98 accum_acc= 0.9666 t= 20.849186182 sec\n",
      "(STEP 2070) loss= 1.5101 acc= 0.96 accum_acc= 0.9659 t= 20.9986031055 sec\n",
      "(STEP 2080) loss= 1.49845 acc= 0.97 accum_acc= 0.9662 t= 21.1036629677 sec\n",
      "(STEP 2090) loss= 1.51956 acc= 0.94 accum_acc= 0.9664 t= 21.1908180714 sec\n",
      "(STEP 2100) loss= 1.49456 acc= 0.98 accum_acc= 0.9673 t= 21.2736740112 sec\n",
      "(STEP 2110) loss= 1.49538 acc= 0.97 accum_acc= 0.9678 t= 21.3408510685 sec\n",
      "(STEP 2120) loss= 1.49045 acc= 0.97 accum_acc= 0.9677 t= 21.4127969742 sec\n",
      "(STEP 2130) loss= 1.4776 acc= 0.99 accum_acc= 0.9675 t= 21.47579813 sec\n",
      "(STEP 2140) loss= 1.48625 acc= 0.98 accum_acc= 0.9679 t= 21.5529539585 sec\n",
      "(STEP 2150) loss= 1.52366 acc= 0.94 accum_acc= 0.9673 t= 21.6279439926 sec\n",
      "(STEP 2160) loss= 1.49559 acc= 0.97 accum_acc= 0.9687 t= 21.7059891224 sec\n",
      "(STEP 2170) loss= 1.5225 acc= 0.94 accum_acc= 0.9693 t= 21.8093049526 sec\n",
      "(STEP 2180) loss= 1.53108 acc= 0.93 accum_acc= 0.969 t= 21.9288060665 sec\n",
      "(STEP 2190) loss= 1.55477 acc= 0.91 accum_acc= 0.9679 t= 22.0391631126 sec\n",
      "(STEP 2200) loss= 1.49739 acc= 0.95 accum_acc= 0.9667 t= 22.1130890846 sec\n",
      "(STEP 2210) loss= 1.486 acc= 0.98 accum_acc= 0.966 t= 22.1842141151 sec\n",
      "(STEP 2220) loss= 1.48969 acc= 0.98 accum_acc= 0.9664 t= 22.2882730961 sec\n",
      "(STEP 2230) loss= 1.48188 acc= 0.98 accum_acc= 0.9665 t= 22.4322359562 sec\n",
      "(STEP 2240) loss= 1.50311 acc= 0.96 accum_acc= 0.9654 t= 22.5625171661 sec\n",
      "(STEP 2250) loss= 1.48798 acc= 0.98 accum_acc= 0.9668 t= 22.6267940998 sec\n",
      "(STEP 2260) loss= 1.49314 acc= 0.97 accum_acc= 0.9668 t= 22.7350940704 sec\n",
      "(STEP 2270) loss= 1.47845 acc= 0.99 accum_acc= 0.967 t= 22.8696081638 sec\n",
      "(STEP 2280) loss= 1.49827 acc= 0.97 accum_acc= 0.9672 t= 22.9505400658 sec\n",
      "(STEP 2290) loss= 1.49907 acc= 0.97 accum_acc= 0.9674 t= 23.0185680389 sec\n",
      "(STEP 2300) loss= 1.48943 acc= 0.97 accum_acc= 0.9685 t= 23.0878031254 sec\n",
      "(STEP 2310) loss= 1.51571 acc= 0.94 accum_acc= 0.9687 t= 23.2349641323 sec\n",
      "(STEP 2320) loss= 1.49097 acc= 0.98 accum_acc= 0.9686 t= 23.3539741039 sec\n",
      "(STEP 2330) loss= 1.48111 acc= 0.98 accum_acc= 0.969 t= 23.4204339981 sec\n",
      "(STEP 2340) loss= 1.49547 acc= 0.97 accum_acc= 0.9687 t= 23.482104063 sec\n",
      "(STEP 2350) loss= 1.48027 acc= 0.99 accum_acc= 0.9692 t= 23.541918993 sec\n",
      "(STEP 2360) loss= 1.5184 acc= 0.94 accum_acc= 0.969 t= 23.6164910793 sec\n",
      "(STEP 2370) loss= 1.50106 acc= 0.96 accum_acc= 0.9675 t= 23.6781871319 sec\n",
      "(STEP 2380) loss= 1.47325 acc= 0.99 accum_acc= 0.9678 t= 23.740926981 sec\n",
      "(STEP 2390) loss= 1.51464 acc= 0.95 accum_acc= 0.9682 t= 23.8033201694 sec\n",
      "(STEP 2399) loss= 1.48786 acc= 0.98 accum_acc= 0.9681 t= 23.8800010681 sec\n",
      "EPOCH: 5\n",
      "(STEP 2400) loss= 1.51846 acc= 0.95 accum_acc= 0.9679 t= 24.3557040691 sec\n",
      "(STEP 2410) loss= 1.4841 acc= 0.98 accum_acc= 0.9685 t= 24.4176471233 sec\n",
      "(STEP 2420) loss= 1.48135 acc= 0.99 accum_acc= 0.9698 t= 24.5319621563 sec\n",
      "-----------------\n",
      "(STEP 2428) loss= 1.46682 acc= 1.0 accum_acc= 0.97 t= 24.6453371048 sec\n",
      "\tTEST ACCURACY= 0.9686\n"
     ]
    }
   ],
   "source": [
    "import fftf\n",
    "import ffbn\n",
    "\n",
    "HIDDEN_1 = 100\n",
    "HIDDEN_2 = 100\n",
    "ALPHA = 0.55\n",
    "USING_BATCH_NORM = True\n",
    "if (USING_BATCH_NORM):\n",
    "    import ffbn as ff\n",
    "else: \n",
    "    import fftf as ff\n",
    "\n",
    "train_x, test_x, train_y, test_y = prepare_data(mnist)\n",
    "print \"training =>\", train_x.shape, train_y.shape\n",
    "print \"testing =>\", test_x.shape, test_y.shape\n",
    "print ff\n",
    "\n",
    "with tf.Graph().as_default():      \n",
    "    img_input = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_PIX])\n",
    "    label_input = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "    model = None\n",
    "    if (USING_BATCH_NORM):\n",
    "        model = ff.inference(img_input, HIDDEN_1, HIDDEN_2, batch_norm_ver=None)\n",
    "    else:\n",
    "        model = ff.inference(img_input, HIDDEN_1, HIDDEN_2)\n",
    "    loss_f = ff.loss(model, label_input)\n",
    "    training_model = ff.training(loss_f, ALPHA)\n",
    "    eval_f = ff.evaluation(model, label_input)\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    loss_arr = []\n",
    "    eval_arr = []\n",
    "    moving_done = False\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        t0 = time.time()\n",
    "        moving_idx = 0\n",
    "        moving_avg = np.zeros(WINDOW_SIZE)\n",
    "        step = 0\n",
    "        for i in xrange(EPOCH):\n",
    "            print \"EPOCH:\", i+1\n",
    "            acc_sum = 0\n",
    "            train_x, train_y = shuffle(train_x, train_y)\n",
    "            for j in xrange(0, TRAINING_SIZE, BATCH_SIZE):\n",
    "                train_x_batch = train_x[j:j+BATCH_SIZE]\n",
    "                train_y_batch = train_y[j:j+BATCH_SIZE]\n",
    "                _, loss, correct = sess.run([training_model, loss_f, eval_f], feed_dict={\n",
    "                    img_input: train_x_batch,\n",
    "                    label_input: train_y_batch\n",
    "                })\n",
    "                \n",
    "                moving_avg[moving_idx] = correct/BATCH_SIZE\n",
    "                moving_idx += 1\n",
    "                if (moving_idx >= WINDOW_SIZE):\n",
    "                    moving_idx = 0\n",
    "                    moving_done = True\n",
    "                \n",
    "                wind_avg= moving_avg.sum()/WINDOW_SIZE\n",
    "                \n",
    "                if(j%1000)==0 or j+BATCH_SIZE == TRAINING_SIZE:\n",
    "                    print \"(STEP %s) loss= %s acc= %s accum_acc= %s t= %s sec\"%(step, loss, correct/BATCH_SIZE, wind_avg, time.time()-t0)\n",
    "                \n",
    "                if (moving_done and wind_avg >= 0.97):\n",
    "                    print \"-----------------\"\n",
    "                    print \"(STEP %s) loss= %s acc= %s accum_acc= %s t= %s sec\"%(step, loss, correct/BATCH_SIZE, wind_avg, time.time()-t0)\n",
    "                    done = True\n",
    "                    break;\n",
    "                step+=1\n",
    "                    \n",
    "            if (done):\n",
    "                break;\n",
    "        num_correct = 0\n",
    "        for k in xrange(0, TEST_SIZE, BATCH_SIZE):\n",
    "            eval_x, eval_y = test_x[k:k+BATCH_SIZE], test_y[k:k+BATCH_SIZE]\n",
    "            if (len(eval_x)!=BATCH_SIZE): \n",
    "                break\n",
    "            correct = sess.run(eval_f, feed_dict={\n",
    "                img_input: eval_x,\n",
    "                label_input: eval_y\n",
    "            })\n",
    "            num_correct += correct\n",
    "#                 print num_correct, correct\n",
    "        print \"\\tTEST ACCURACY= %s\"%(num_correct/TEST_SIZE)\n",
    "                    \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "# ds = input_data.read_data_sets(\"data\", validation_size=0)\n",
    "\n",
    "# print ds.train.epochs_completed\n",
    "# print ds.train.images.shape\n",
    "\n",
    "# print train_y[0]==ds.train.labels[1]\n",
    "# print set(ds.train.images[1]) == set(train_x[0])\n",
    "\n",
    "# plt.subplot(3,3,1)\n",
    "# plt.imshow(train_x[0].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,2)\n",
    "# plt.imshow(train_x[1].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,3)\n",
    "# plt.imshow(train_x[2].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,4)\n",
    "# plt.imshow(train_x[3].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,5)\n",
    "# plt.imshow(train_x[4].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,6)\n",
    "# plt.imshow(train_x[5].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,7)\n",
    "# plt.imshow(train_x[6].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,8)\n",
    "# plt.imshow(train_x[7].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,9)\n",
    "# plt.imshow(train_x[8].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "\n",
    "# # plt.subplot(1,2,2)\n",
    "# # plt.imshow(ds.train.images[3].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# # print train_y[2]\n",
    "# # print ds.train.labels[3]\n",
    "\n",
    "# print type(ds.train.labels[0])\n",
    "# print train_y[:10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
