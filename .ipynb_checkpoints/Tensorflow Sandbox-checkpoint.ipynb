{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NatchaS/anaconda/envs/tensorflow/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_PIX: 784\n",
      "IMG_SIZE: 28\n",
      "TOTAL_SIZE: 70000\n",
      "\n",
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import sklearn.datasets as sk_data\n",
    "import math\n",
    "import time\n",
    "mnist = fetch_mldata('MNIST original', data_home=sk_data.get_data_home())\n",
    "\n",
    "IMG_PIX = mnist.data.shape[1]\n",
    "IMG_SIZE = int(math.sqrt(IMG_PIX))\n",
    "TRAINING_SIZE = 60000\n",
    "TEST_SIZE = 10000\n",
    "TOTAL_SIZE = mnist.data.shape[0]\n",
    "EPOCH = 50\n",
    "BATCH_SIZE = 100\n",
    "WINDOW_SIZE = 10000/BATCH_SIZE\n",
    "\n",
    "print \"IMG_PIX: %s\\nIMG_SIZE: %s\\nTOTAL_SIZE: %s\\n\"%(IMG_PIX, IMG_SIZE, TOTAL_SIZE)\n",
    "\n",
    "def vectorize_y(Y):\n",
    "    v_y = np.zeros((Y.shape[0], max(set(Y.flatten()))+1))\n",
    "    print v_y.shape\n",
    "    for i in range(len(Y)):\n",
    "        v_y[i][Y[i]] = 1.0\n",
    "    return v_y\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    Y = np.array([Y.flatten()]).T\n",
    "    A = np.concatenate((X, Y),axis=1)\n",
    "    np.random.shuffle(A)\n",
    "    X_prime, Y_prime = np.hsplit(A, [-1])\n",
    "#     if (set(Y_prime.flatten())!=set([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0])):\n",
    "#         return shuffle(X,Y)\n",
    "    Y_prime = Y_prime.reshape(Y_prime.shape[0],)\n",
    "    return X_prime, Y_prime\n",
    "\n",
    "def prepare_data(_mnist):\n",
    "    _mnist.data = _mnist.data.astype(np.float32)\n",
    "    _mnist.data = np.multiply(_mnist.data, 1.0 / 255.0)\n",
    "    _mnist.target = _mnist.target.astype(np.uint8)\n",
    "    train_xx, test_xx = np.vsplit(_mnist.data, [TRAINING_SIZE])\n",
    "    train_yy, test_yy = np.array_split(_mnist.target, [TRAINING_SIZE])\n",
    "    return train_xx, test_xx, train_yy, test_yy\n",
    "\n",
    "print type(mnist.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training => (60000, 784) (60000,)\n",
      "testing => (10000, 784) (10000,)\n",
      "<module 'fftf' from 'fftf.pyc'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NatchaS/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:42: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 0 - 100 at 00 clocks and 0.523190021515 sec : classification 0.11 / 100.0 = 0.11 cross entropy error 2.30056\n",
      "training 1000 - 1100 at 00 clocks and 0.56488609314 sec : classification 3.78 / 100.0 = 0.343636363636 cross entropy error 1.46168\n",
      "training 2000 - 2100 at 00 clocks and 0.6115899086 sec : classification 8.5 / 100.0 = 0.404761904762 cross entropy error 1.30723\n",
      "training 3000 - 3100 at 00 clocks and 0.651308059692 sec : classification 14.71 / 100.0 = 0.474516129032 cross entropy error 1.09581\n",
      "training 4000 - 4100 at 00 clocks and 0.711420059204 sec : classification 22.16 / 100.0 = 0.540487804878 cross entropy error 0.593504\n",
      "training 5000 - 5100 at 00 clocks and 0.760569095612 sec : classification 30.48 / 100.0 = 0.597647058824 cross entropy error 0.721819\n",
      "training 6000 - 6100 at 00 clocks and 0.807677030563 sec : classification 38.67 / 100.0 = 0.63393442623 cross entropy error 0.36779\n",
      "training 7000 - 7100 at 00 clocks and 0.847009897232 sec : classification 47.09 / 100.0 = 0.66323943662 cross entropy error 0.530114\n",
      "training 8000 - 8100 at 00 clocks and 0.888710975647 sec : classification 55.7 / 100.0 = 0.687654320988 cross entropy error 0.336158\n",
      "training 9000 - 9100 at 00 clocks and 0.932851076126 sec : classification 64.27 / 100.0 = 0.706263736264 cross entropy error 0.592086\n",
      "training 10000 - 10100 at 00 clocks and 0.985194921494 sec : classification 72.89 / 100.0 = 0.7289 cross entropy error 0.390045\n",
      "training 11000 - 11100 at 00 clocks and 1.02857303619 sec : classification 78.24 / 100.0 = 0.7824 cross entropy error 0.252203\n",
      "training 12000 - 12100 at 00 clocks and 1.07216286659 sec : classification 81.47 / 100.0 = 0.8147 cross entropy error 0.447739\n",
      "training 13000 - 13100 at 00 clocks and 1.12797093391 sec : classification 84.25 / 100.0 = 0.8425 cross entropy error 0.390863\n",
      "training 14000 - 14100 at 00 clocks and 1.16937398911 sec : classification 85.73 / 100.0 = 0.8573 cross entropy error 0.482974\n",
      "training 15000 - 15100 at 00 clocks and 1.22272586823 sec : classification 86.04 / 100.0 = 0.8604 cross entropy error 0.407364\n",
      "training 16000 - 16100 at 00 clocks and 1.26969099045 sec : classification 86.94 / 100.0 = 0.8694 cross entropy error 0.376324\n",
      "training 17000 - 17100 at 00 clocks and 1.32459592819 sec : classification 87.72 / 100.0 = 0.8772 cross entropy error 0.224976\n",
      "training 18000 - 18100 at 00 clocks and 1.3630669117 sec : classification 88.24 / 100.0 = 0.8824 cross entropy error 0.299873\n",
      "training 19000 - 19100 at 00 clocks and 1.4124250412 sec : classification 88.75 / 100.0 = 0.8875 cross entropy error 0.258993\n",
      "training 20000 - 20100 at 00 clocks and 1.45475506783 sec : classification 89.28 / 100.0 = 0.8928 cross entropy error 0.279312\n",
      "training 21000 - 21100 at 00 clocks and 1.49978590012 sec : classification 89.48 / 100.0 = 0.8948 cross entropy error 0.224802\n",
      "training 22000 - 22100 at 00 clocks and 1.55133700371 sec : classification 90.86 / 100.0 = 0.9086 cross entropy error 0.198282\n",
      "training 23000 - 23100 at 00 clocks and 1.59633088112 sec : classification 91.02 / 100.0 = 0.9102 cross entropy error 0.286525\n",
      "training 24000 - 24100 at 00 clocks and 1.64004206657 sec : classification 91.32 / 100.0 = 0.9132 cross entropy error 0.201952\n",
      "training 25000 - 25100 at 00 clocks and 1.69097995758 sec : classification 91.93 / 100.0 = 0.9193 cross entropy error 0.24669\n",
      "training 26000 - 26100 at 00 clocks and 1.74180102348 sec : classification 92.17 / 100.0 = 0.9217 cross entropy error 0.24636\n",
      "training 27000 - 27100 at 00 clocks and 1.78022289276 sec : classification 92.24 / 100.0 = 0.9224 cross entropy error 0.22438\n",
      "training 28000 - 28100 at 00 clocks and 1.81779289246 sec : classification 92.34 / 100.0 = 0.9234 cross entropy error 0.190931\n",
      "training 29000 - 29100 at 00 clocks and 1.85378694534 sec : classification 92.66 / 100.0 = 0.9266 cross entropy error 0.338498\n",
      "training 30000 - 30100 at 00 clocks and 1.88960695267 sec : classification 92.71 / 100.0 = 0.9271 cross entropy error 0.152768\n",
      "training 31000 - 31100 at 00 clocks and 1.93382406235 sec : classification 92.83 / 100.0 = 0.9283 cross entropy error 0.276199\n",
      "training 32000 - 32100 at 00 clocks and 1.96727609634 sec : classification 92.8 / 100.0 = 0.928 cross entropy error 0.2865\n",
      "training 33000 - 33100 at 00 clocks and 2.00269508362 sec : classification 93.1 / 100.0 = 0.931 cross entropy error 0.197319\n",
      "training 34000 - 34100 at 00 clocks and 2.0620598793 sec : classification 93.33 / 100.0 = 0.9333 cross entropy error 0.320687\n",
      "training 35000 - 35100 at 00 clocks and 2.14556097984 sec : classification 93.49 / 100.0 = 0.9349 cross entropy error 0.26037\n",
      "training 36000 - 36100 at 00 clocks and 2.21119809151 sec : classification 93.43 / 100.0 = 0.9343 cross entropy error 0.229997\n",
      "training 37000 - 37100 at 00 clocks and 2.26233005524 sec : classification 93.46 / 100.0 = 0.9346 cross entropy error 0.251277\n",
      "training 38000 - 38100 at 00 clocks and 2.30485105515 sec : classification 93.58 / 100.0 = 0.9358 cross entropy error 0.295169\n",
      "training 39000 - 39100 at 00 clocks and 2.35831904411 sec : classification 93.45 / 100.0 = 0.9345 cross entropy error 0.203156\n",
      "training 40000 - 40100 at 00 clocks and 2.41229200363 sec : classification 93.54 / 100.0 = 0.9354 cross entropy error 0.265924\n",
      "training 41000 - 41100 at 00 clocks and 2.45451807976 sec : classification 93.62 / 100.0 = 0.9362 cross entropy error 0.209411\n",
      "training 42000 - 42100 at 00 clocks and 2.50767493248 sec : classification 93.7 / 100.0 = 0.937 cross entropy error 0.3089\n",
      "training 43000 - 43100 at 00 clocks and 2.55533599854 sec : classification 93.63 / 100.0 = 0.9363 cross entropy error 0.166556\n",
      "training 44000 - 44100 at 00 clocks and 2.62028503418 sec : classification 93.7 / 100.0 = 0.937 cross entropy error 0.177677\n",
      "training 45000 - 45100 at 00 clocks and 2.66140508652 sec : classification 93.64 / 100.0 = 0.9364 cross entropy error 0.192711\n",
      "training 46000 - 46100 at 00 clocks and 2.7171049118 sec : classification 93.89 / 100.0 = 0.9389 cross entropy error 0.302181\n",
      "training 47000 - 47100 at 00 clocks and 2.75812387466 sec : classification 93.96 / 100.0 = 0.9396 cross entropy error 0.0944352\n",
      "training 48000 - 48100 at 00 clocks and 2.81242990494 sec : classification 93.92 / 100.0 = 0.9392 cross entropy error 0.27005\n",
      "training 49000 - 49100 at 00 clocks and 2.85632491112 sec : classification 94.18 / 100.0 = 0.9418 cross entropy error 0.106596\n",
      "training 50000 - 50100 at 00 clocks and 2.90593791008 sec : classification 94.18 / 100.0 = 0.9418 cross entropy error 0.113725\n",
      "training 51000 - 51100 at 00 clocks and 2.94746494293 sec : classification 94.25 / 100.0 = 0.9425 cross entropy error 0.171073\n",
      "training 52000 - 52100 at 00 clocks and 2.98640608788 sec : classification 94.23 / 100.0 = 0.9423 cross entropy error 0.320467\n",
      "training 53000 - 53100 at 00 clocks and 3.02973794937 sec : classification 94.25 / 100.0 = 0.9425 cross entropy error 0.190354\n",
      "training 54000 - 54100 at 00 clocks and 3.08014297485 sec : classification 94.16 / 100.0 = 0.9416 cross entropy error 0.158628\n",
      "training 55000 - 55100 at 00 clocks and 3.13524699211 sec : classification 94.25 / 100.0 = 0.9425 cross entropy error 0.0658012\n",
      "training 56000 - 56100 at 00 clocks and 3.17600798607 sec : classification 94.21 / 100.0 = 0.9421 cross entropy error 0.0621222\n",
      "training 57000 - 57100 at 00 clocks and 3.22117400169 sec : classification 94.16 / 100.0 = 0.9416 cross entropy error 0.176179\n",
      "training 58000 - 58100 at 00 clocks and 3.26454401016 sec : classification 94.23 / 100.0 = 0.9423 cross entropy error 0.134664\n",
      "training 59000 - 59100 at 00 clocks and 3.31230092049 sec : classification 94.25 / 100.0 = 0.9425 cross entropy error 0.180586\n",
      "training 59900 - 60000 at 00 clocks and 3.34991002083 sec : classification 94.2 / 100.0 = 0.942 cross entropy error 0.138901\n",
      "training 0 - 100 at 00 clocks and 4.06485104561 sec : classification 94.19 / 100.0 = 0.9419 cross entropy error 0.190086\n",
      "training 1000 - 1100 at 00 clocks and 4.18076300621 sec : classification 94.03 / 100.0 = 0.9403 cross entropy error 0.136674\n",
      "training 2000 - 2100 at 00 clocks and 4.2425429821 sec : classification 94.16 / 100.0 = 0.9416 cross entropy error 0.0597617\n",
      "training 3000 - 3100 at 00 clocks and 4.27957391739 sec : classification 94.29 / 100.0 = 0.9429 cross entropy error 0.13277\n",
      "training 4000 - 4100 at 00 clocks and 4.34087586403 sec : classification 94.38 / 100.0 = 0.9438 cross entropy error 0.0834384\n",
      "training 5000 - 5100 at 00 clocks and 4.38668298721 sec : classification 94.6 / 100.0 = 0.946 cross entropy error 0.0409306\n",
      "training 6000 - 6100 at 00 clocks and 4.47189593315 sec : classification 94.51 / 100.0 = 0.9451 cross entropy error 0.281146\n",
      "training 7000 - 7100 at 00 clocks and 4.53081989288 sec : classification 94.66 / 100.0 = 0.9466 cross entropy error 0.270935\n",
      "training 8000 - 8100 at 00 clocks and 4.5972969532 sec : classification 94.8 / 100.0 = 0.948 cross entropy error 0.141631\n",
      "training 9000 - 9100 at 00 clocks and 4.65444087982 sec : classification 94.86 / 100.0 = 0.9486 cross entropy error 0.0487116\n",
      "training 10000 - 10100 at 00 clocks and 4.71039509773 sec : classification 95.06 / 100.0 = 0.9506 cross entropy error 0.162577\n",
      "training 11000 - 11100 at 00 clocks and 4.76565504074 sec : classification 95.3 / 100.0 = 0.953 cross entropy error 0.240665\n",
      "training 12000 - 12100 at 00 clocks and 4.81592297554 sec : classification 95.44 / 100.0 = 0.9544 cross entropy error 0.0957132\n",
      "training 13000 - 13100 at 00 clocks and 4.86991786957 sec : classification 95.44 / 100.0 = 0.9544 cross entropy error 0.360568\n",
      "training 14000 - 14100 at 00 clocks and 4.91401004791 sec : classification 95.62 / 100.0 = 0.9562 cross entropy error 0.137152\n",
      "training 15000 - 15100 at 00 clocks and 4.97133994102 sec : classification 95.47 / 100.0 = 0.9547 cross entropy error 0.249549\n",
      "training 16000 - 16100 at 00 clocks and 5.03075003624 sec : classification 95.71 / 100.0 = 0.9571 cross entropy error 0.277709\n",
      "training 17000 - 17100 at 00 clocks and 5.07556390762 sec : classification 95.71 / 100.0 = 0.9571 cross entropy error 0.148746\n",
      "training 18000 - 18100 at 00 clocks and 5.12042498589 sec : classification 95.85 / 100.0 = 0.9585 cross entropy error 0.0895455\n",
      "training 19000 - 19100 at 00 clocks and 5.16610693932 sec : classification 95.89 / 100.0 = 0.9589 cross entropy error 0.0957671\n",
      "training 20000 - 20100 at 00 clocks and 5.21678209305 sec : classification 95.95 / 100.0 = 0.9595 cross entropy error 0.0591048\n",
      "training 21000 - 21100 at 00 clocks and 5.27456998825 sec : classification 95.94 / 100.0 = 0.9594 cross entropy error 0.248845\n",
      "training 22000 - 22100 at 00 clocks and 5.32645797729 sec : classification 95.8 / 100.0 = 0.958 cross entropy error 0.289221\n",
      "training 23000 - 23100 at 00 clocks and 5.37648797035 sec : classification 95.88 / 100.0 = 0.9588 cross entropy error 0.0471695\n",
      "training 24000 - 24100 at 00 clocks and 5.42711305618 sec : classification 95.77 / 100.0 = 0.9577 cross entropy error 0.0621204\n",
      "training 25000 - 25100 at 00 clocks and 5.47266793251 sec : classification 95.81 / 100.0 = 0.9581 cross entropy error 0.114501\n",
      "training 26000 - 26100 at 00 clocks and 5.50929093361 sec : classification 95.74 / 100.0 = 0.9574 cross entropy error 0.166268\n",
      "training 27000 - 27100 at 00 clocks and 5.54676795006 sec : classification 95.95 / 100.0 = 0.9595 cross entropy error 0.0465701\n",
      "training 28000 - 28100 at 00 clocks and 5.581594944 sec : classification 95.92 / 100.0 = 0.9592 cross entropy error 0.0974018\n",
      "training 29000 - 29100 at 00 clocks and 5.61655402184 sec : classification 95.9 / 100.0 = 0.959 cross entropy error 0.10919\n",
      "training 30000 - 30100 at 00 clocks and 5.66551589966 sec : classification 95.91 / 100.0 = 0.9591 cross entropy error 0.102491\n",
      "training 31000 - 31100 at 00 clocks and 5.7162270546 sec : classification 95.91 / 100.0 = 0.9591 cross entropy error 0.104788\n",
      "training 32000 - 32100 at 00 clocks and 5.76691508293 sec : classification 96.04 / 100.0 = 0.9604 cross entropy error 0.0441545\n",
      "training 33000 - 33100 at 00 clocks and 5.8050339222 sec : classification 95.93 / 100.0 = 0.9593 cross entropy error 0.0928256\n",
      "training 34000 - 34100 at 00 clocks and 5.83949995041 sec : classification 95.85 / 100.0 = 0.9585 cross entropy error 0.145071\n",
      "training 35000 - 35100 at 00 clocks and 5.87273907661 sec : classification 95.89 / 100.0 = 0.9589 cross entropy error 0.149793\n",
      "training 36000 - 36100 at 00 clocks and 5.92233109474 sec : classification 95.93 / 100.0 = 0.9593 cross entropy error 0.100958\n",
      "training 37000 - 37100 at 00 clocks and 5.95316386223 sec : classification 95.76 / 100.0 = 0.9576 cross entropy error 0.144988\n",
      "training 38000 - 38100 at 00 clocks and 5.98518586159 sec : classification 95.81 / 100.0 = 0.9581 cross entropy error 0.0411219\n",
      "training 39000 - 39100 at 00 clocks and 6.02960395813 sec : classification 95.85 / 100.0 = 0.9585 cross entropy error 0.0981638\n",
      "training 40000 - 40100 at 00 clocks and 6.10213398933 sec : classification 95.83 / 100.0 = 0.9583 cross entropy error 0.129667\n",
      "training 41000 - 41100 at 00 clocks and 6.16396808624 sec : classification 95.83 / 100.0 = 0.9583 cross entropy error 0.167714\n",
      "training 42000 - 42100 at 00 clocks and 6.20080900192 sec : classification 95.85 / 100.0 = 0.9585 cross entropy error 0.103224\n",
      "training 43000 - 43100 at 00 clocks and 6.23557496071 sec : classification 96.0 / 100.0 = 0.96 cross entropy error 0.0933313\n",
      "training 44000 - 44100 at 00 clocks and 6.2821199894 sec : classification 96.1 / 100.0 = 0.961 cross entropy error 0.171963\n",
      "training 45000 - 45100 at 00 clocks and 6.33883905411 sec : classification 96.0 / 100.0 = 0.96 cross entropy error 0.111431\n",
      "training 46000 - 46100 at 00 clocks and 6.39289999008 sec : classification 96.04 / 100.0 = 0.9604 cross entropy error 0.14282\n",
      "training 47000 - 47100 at 00 clocks and 6.43908190727 sec : classification 96.07 / 100.0 = 0.9607 cross entropy error 0.124247\n",
      "training 48000 - 48100 at 00 clocks and 6.47734308243 sec : classification 96.1 / 100.0 = 0.961 cross entropy error 0.0544355\n",
      "training 49000 - 49100 at 00 clocks and 6.52909088135 sec : classification 96.09 / 100.0 = 0.9609 cross entropy error 0.0504868\n",
      "training 50000 - 50100 at 00 clocks and 6.58006596565 sec : classification 96.08 / 100.0 = 0.9608 cross entropy error 0.212099\n",
      "training 51000 - 51100 at 00 clocks and 6.62768292427 sec : classification 95.89 / 100.0 = 0.9589 cross entropy error 0.171996\n",
      "training 52000 - 52100 at 00 clocks and 6.69361591339 sec : classification 95.95 / 100.0 = 0.9595 cross entropy error 0.100942\n",
      "training 53000 - 53100 at 00 clocks and 6.73710107803 sec : classification 95.88 / 100.0 = 0.9588 cross entropy error 0.140574\n",
      "training 54000 - 54100 at 00 clocks and 6.80541205406 sec : classification 95.91 / 100.0 = 0.9591 cross entropy error 0.100288\n",
      "training 55000 - 55100 at 00 clocks and 6.84404206276 sec : classification 95.99 / 100.0 = 0.9599 cross entropy error 0.29419\n",
      "training 56000 - 56100 at 00 clocks and 6.88295197487 sec : classification 95.93 / 100.0 = 0.9593 cross entropy error 0.19572\n",
      "training 57000 - 57100 at 00 clocks and 6.91455698013 sec : classification 96.02 / 100.0 = 0.9602 cross entropy error 0.0719162\n",
      "training 58000 - 58100 at 00 clocks and 6.94623303413 sec : classification 95.86 / 100.0 = 0.9586 cross entropy error 0.108584\n",
      "training 59000 - 59100 at 00 clocks and 6.98005509377 sec : classification 95.81 / 100.0 = 0.9581 cross entropy error 0.0857241\n",
      "training 59900 - 60000 at 00 clocks and 7.00752091408 sec : classification 95.73 / 100.0 = 0.9573 cross entropy error 0.161286\n",
      "training 0 - 100 at 00 clocks and 7.56224799156 sec : classification 95.7 / 100.0 = 0.957 cross entropy error 0.181279\n",
      "training 1000 - 1100 at 00 clocks and 7.59770798683 sec : classification 96.03 / 100.0 = 0.9603 cross entropy error 0.136279\n",
      "training 2000 - 2100 at 00 clocks and 7.63559699059 sec : classification 95.99 / 100.0 = 0.9599 cross entropy error 0.148717\n",
      "training 3000 - 3100 at 00 clocks and 7.67634296417 sec : classification 96.11 / 100.0 = 0.9611 cross entropy error 0.138568\n",
      "training 4000 - 4100 at 00 clocks and 7.72181892395 sec : classification 96.2 / 100.0 = 0.962 cross entropy error 0.104385\n",
      "training 5000 - 5100 at 00 clocks and 7.75823402405 sec : classification 96.36 / 100.0 = 0.9636 cross entropy error 0.100811\n",
      "training 6000 - 6100 at 00 clocks and 7.80171704292 sec : classification 96.47 / 100.0 = 0.9647 cross entropy error 0.0762424\n",
      "training 7000 - 7100 at 00 clocks and 7.83546686172 sec : classification 96.49 / 100.0 = 0.9649 cross entropy error 0.167635\n",
      "training 8000 - 8100 at 00 clocks and 7.87444186211 sec : classification 96.55 / 100.0 = 0.9655 cross entropy error 0.0239445\n",
      "training 9000 - 9100 at 00 clocks and 7.91202998161 sec : classification 96.56 / 100.0 = 0.9656 cross entropy error 0.12943\n",
      "training 10000 - 10100 at 00 clocks and 7.94942998886 sec : classification 96.72 / 100.0 = 0.9672 cross entropy error 0.299881\n",
      "training 11000 - 11100 at 00 clocks and 7.98421192169 sec : classification 96.83 / 100.0 = 0.9683 cross entropy error 0.0339423\n",
      "training 12000 - 12100 at 00 clocks and 8.02381086349 sec : classification 96.71 / 100.0 = 0.9671 cross entropy error 0.0792957\n",
      "training 13000 - 13100 at 00 clocks and 8.09678506851 sec : classification 96.57 / 100.0 = 0.9657 cross entropy error 0.261401\n",
      "training 14000 - 14100 at 00 clocks and 8.16899108887 sec : classification 96.42 / 100.0 = 0.9642 cross entropy error 0.134698\n",
      "training 15000 - 15100 at 00 clocks and 8.21492600441 sec : classification 96.28 / 100.0 = 0.9628 cross entropy error 0.0512478\n",
      "training 16000 - 16100 at 00 clocks and 8.25693488121 sec : classification 96.18 / 100.0 = 0.9618 cross entropy error 0.0837086\n",
      "training 17000 - 17100 at 00 clocks and 8.30169606209 sec : classification 96.2 / 100.0 = 0.962 cross entropy error 0.113768\n",
      "training 18000 - 18100 at 00 clocks and 8.34734606743 sec : classification 96.27 / 100.0 = 0.9627 cross entropy error 0.161982\n",
      "training 19000 - 19100 at 00 clocks and 8.3906788826 sec : classification 96.3 / 100.0 = 0.963 cross entropy error 0.1241\n",
      "training 20000 - 20100 at 00 clocks and 8.43292307854 sec : classification 96.37 / 100.0 = 0.9637 cross entropy error 0.18149\n",
      "training 21000 - 21100 at 00 clocks and 8.47900700569 sec : classification 96.28 / 100.0 = 0.9628 cross entropy error 0.0626574\n",
      "training 22000 - 22100 at 00 clocks and 8.52237200737 sec : classification 96.29 / 100.0 = 0.9629 cross entropy error 0.0411924\n",
      "training 23000 - 23100 at 00 clocks and 8.57320094109 sec : classification 96.35 / 100.0 = 0.9635 cross entropy error 0.220563\n",
      "training 24000 - 24100 at 00 clocks and 8.62070894241 sec : classification 96.49 / 100.0 = 0.9649 cross entropy error 0.0954554\n",
      "training 25000 - 25100 at 00 clocks and 8.66833305359 sec : classification 96.64 / 100.0 = 0.9664 cross entropy error 0.14985\n",
      "training 26000 - 26100 at 00 clocks and 8.72972106934 sec : classification 96.75 / 100.0 = 0.9675 cross entropy error 0.142625\n",
      "training 27000 - 27100 at 00 clocks and 8.77903103828 sec : classification 96.73 / 100.0 = 0.9673 cross entropy error 0.132233\n",
      "training 28000 - 28100 at 00 clocks and 8.82886695862 sec : classification 96.67 / 100.0 = 0.9667 cross entropy error 0.0983817\n",
      "training 29000 - 29100 at 00 clocks and 8.88090395927 sec : classification 96.6 / 100.0 = 0.966 cross entropy error 0.0540868\n",
      "training 30000 - 30100 at 00 clocks and 8.92920994759 sec : classification 96.54 / 100.0 = 0.9654 cross entropy error 0.102865\n",
      "training 31000 - 31100 at 00 clocks and 8.97001600266 sec : classification 96.42 / 100.0 = 0.9642 cross entropy error 0.187665\n",
      "training 32000 - 32100 at 00 clocks and 9.01072692871 sec : classification 96.55 / 100.0 = 0.9655 cross entropy error 0.0891828\n",
      "training 33000 - 33100 at 00 clocks and 9.05480408669 sec : classification 96.71 / 100.0 = 0.9671 cross entropy error 0.056761\n",
      "training 34000 - 34100 at 00 clocks and 9.09098100662 sec : classification 96.71 / 100.0 = 0.9671 cross entropy error 0.0872303\n",
      "training 35000 - 35100 at 00 clocks and 9.13696193695 sec : classification 96.67 / 100.0 = 0.9667 cross entropy error 0.0728792\n",
      "training 36000 - 36100 at 00 clocks and 9.17689704895 sec : classification 96.74 / 100.0 = 0.9674 cross entropy error 0.112856\n",
      "training 37000 - 37100 at 00 clocks and 9.22247886658 sec : classification 96.73 / 100.0 = 0.9673 cross entropy error 0.111963\n",
      "training 38000 - 38100 at 00 clocks and 9.26242399216 sec : classification 96.77 / 100.0 = 0.9677 cross entropy error 0.174366\n",
      "training 39000 - 39100 at 00 clocks and 9.30103492737 sec : classification 96.92 / 100.0 = 0.9692 cross entropy error 0.0795709\n",
      "training 40000 - 40100 at 00 clocks and 9.34741902351 sec : classification 96.94 / 100.0 = 0.9694 cross entropy error 0.12284\n",
      "training 40300 - 40400 at 00 clocks and 9.36066007614 sec : classification 97.03 / 100.0 = 0.9703 cross entropy error 0.0951816\n",
      "test_sample_start:0\n",
      "test_sample_end:10000\n",
      "interval between reports:100\n",
      "train_sec:9.36066007614\n",
      "test_sec:0.239261865616\n",
      "test_correct:0.9631\n",
      "samples for testing:10000\n",
      "hidden units:64\n",
      "mini batch size:100\n",
      "average window size:100.0\n",
      "samples for training:60000\n",
      "language:python\n",
      "output_classes:10\n",
      "pixels per image:784\n",
      "library:TensorFlow\n",
      "processor:i5-7500\n"
     ]
    }
   ],
   "source": [
    "import fftf\n",
    "import ffbn\n",
    "\n",
    "HIDDEN_1 = 64\n",
    "HIDDEN_2 = 64\n",
    "ALPHA = 0.5\n",
    "USING_BATCH_NORM = False\n",
    "\n",
    "if (USING_BATCH_NORM):\n",
    "    import ffbn as ff\n",
    "else: \n",
    "    import fftf as ff\n",
    "\n",
    "train_x, test_x, train_y, test_y = prepare_data(mnist)\n",
    "print \"training =>\", train_x.shape, train_y.shape\n",
    "print \"testing =>\", test_x.shape, test_y.shape\n",
    "print ff\n",
    "\n",
    "with tf.Graph().as_default(), tf.device('/cpu:0'):      \n",
    "    img_input = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_PIX])\n",
    "    label_input = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "    model = None\n",
    "    if (USING_BATCH_NORM):\n",
    "        model = ff.inference(img_input, HIDDEN_1, HIDDEN_2, batch_norm_ver=None)\n",
    "    else:\n",
    "        model = ff.inference(img_input, HIDDEN_1, HIDDEN_2)\n",
    "    loss_f = ff.loss(model, label_input)\n",
    "    # 'adagrad', 'adam', 'momentum', 'default'\n",
    "    training_model = ff.training(loss_f, ALPHA, optimizer=\"momentum\", momentum=0.1)\n",
    "    eval_f = ff.evaluation(model, label_input)\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    loss_arr = []\n",
    "    eval_arr = []\n",
    "    output = []\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement = True)) as sess:\n",
    "        sess.run(init)\n",
    "        moving_idx = 0\n",
    "        moving_avg = np.zeros(WINDOW_SIZE)\n",
    "        still_moving = 1\n",
    "        t0 = time.time()\n",
    "        t1 = t0\n",
    "        for i in xrange(EPOCH):\n",
    "            #print \"EPOCH:\", i+1\n",
    "            acc_sum = 0\n",
    "            train_x, train_y = shuffle(train_x, train_y)\n",
    "            for j in xrange(0, TRAINING_SIZE, BATCH_SIZE):\n",
    "                z = j + BATCH_SIZE\n",
    "                train_x_batch = train_x[j:z]\n",
    "                train_y_batch = train_y[j:z]\n",
    "                _, loss, correct = sess.run([training_model, loss_f, eval_f], feed_dict={\n",
    "                    img_input: train_x_batch,\n",
    "                    label_input: train_y_batch\n",
    "                })\n",
    "                \n",
    "                moving_avg[moving_idx] = correct/len(train_x_batch)\n",
    "                moving_idx += 1\n",
    "                if (still_moving and moving_idx >= WINDOW_SIZE):\n",
    "                    moving_idx = 0\n",
    "                    still_moving = 0\n",
    "                elif (moving_idx >= WINDOW_SIZE):\n",
    "                    moving_idx = 0\n",
    "                \n",
    "                sum = moving_avg.sum()\n",
    "                if (still_moving):\n",
    "                    wind_avg = sum/(moving_idx*BATCH_SIZE) * 100\n",
    "                else:\n",
    "                    wind_avg= sum/WINDOW_SIZE\n",
    "                #wind_avg=correct/BATCH_SIZE\n",
    "                \n",
    "                if(j%1000)==0 or j+BATCH_SIZE == TRAINING_SIZE:\n",
    "                    print(\"training %d - %d at 00 clocks and %s sec : classification %s / %s = %s cross entropy error %s\"%(z-WINDOW_SIZE,z,time.time()-t0,sum,WINDOW_SIZE,wind_avg,loss))\n",
    "\n",
    "                if (wind_avg >= 0.97):\n",
    "                    t1 = time.time()-t0\n",
    "                    print(\"training %d - %d at 00 clocks and %s sec : classification %s / %s = %s cross entropy error %s\"%(z-WINDOW_SIZE,z,t1,sum,WINDOW_SIZE,wind_avg,loss))\n",
    "                    done = True\n",
    "                    break;\n",
    "                    \n",
    "            if (done):\n",
    "                for i in output:\n",
    "                    print i\n",
    "                print \"test_sample_start:0\"\n",
    "                print \"test_sample_end:%s\"%(TEST_SIZE)\n",
    "                print \"interval between reports:%s\"%(BATCH_SIZE)\n",
    "                print \"train_sec:%s\"%(t1)\n",
    "                break;\n",
    "        num_correct = 0\n",
    "        t0 = time.time()\n",
    "        for k in xrange(0, TEST_SIZE, BATCH_SIZE):\n",
    "            eval_x, eval_y = test_x[k:k+BATCH_SIZE], test_y[k:k+BATCH_SIZE]\n",
    "            if (len(eval_x)!=BATCH_SIZE): \n",
    "                break\n",
    "            correct = sess.run(eval_f, feed_dict={\n",
    "                img_input: eval_x,\n",
    "                label_input: eval_y\n",
    "            })\n",
    "            num_correct += correct\n",
    "        t1 = time.time()-t0\n",
    "        print \"test_sec:%s\"%(t1)\n",
    "        print \"test_correct:%s\"%(num_correct/TEST_SIZE)\n",
    "        print \"samples for testing:%s\"%(TEST_SIZE)\n",
    "        print \"hidden units:%s\"%(HIDDEN_1)\n",
    "        print \"mini batch size:%s\"%(BATCH_SIZE)\n",
    "        print \"average window size:%s\"%(WINDOW_SIZE)\n",
    "        print \"samples for training:%s\"%(TRAINING_SIZE)\n",
    "        print \"language:python\"\n",
    "        print \"output_classes:10\"\n",
    "        print \"pixels per image:784\"\n",
    "        print \"library:TensorFlow\"\n",
    "        print \"processor:i5-7500\"\n",
    "        #print \"early stop: true\"\n",
    "                    \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "# ds = input_data.read_data_sets(\"data\", validation_size=0)\n",
    "\n",
    "# print ds.train.epochs_completed\n",
    "# print ds.train.images.shape\n",
    "\n",
    "# print train_y[0]==ds.train.labels[1]\n",
    "# print set(ds.train.images[1]) == set(train_x[0])\n",
    "\n",
    "# plt.subplot(3,3,1)\n",
    "# plt.imshow(train_x[0].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,2)\n",
    "# plt.imshow(train_x[1].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,3)\n",
    "# plt.imshow(train_x[2].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,4)\n",
    "# plt.imshow(train_x[3].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,5)\n",
    "# plt.imshow(train_x[4].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,6)\n",
    "# plt.imshow(train_x[5].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,7)\n",
    "# plt.imshow(train_x[6].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,8)\n",
    "# plt.imshow(train_x[7].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,9)\n",
    "# plt.imshow(train_x[8].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "\n",
    "# # plt.subplot(1,2,2)\n",
    "# # plt.imshow(ds.train.images[3].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# # print train_y[2]\n",
    "# # print ds.train.labels[3]\n",
    "\n",
    "# print type(ds.train.labels[0])\n",
    "# print train_y[:10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
