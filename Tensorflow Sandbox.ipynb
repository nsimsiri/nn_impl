{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_PIX: 784\n",
      "IMG_SIZE: 28\n",
      "TOTAL_SIZE: 70000\n",
      "\n",
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import sklearn.datasets as sk_data\n",
    "import math\n",
    "import time\n",
    "#d afdsf\n",
    "mnist = fetch_mldata('MNIST original', data_home=sk_data.get_data_home())\n",
    "\n",
    "IMG_PIX = mnist.data.shape[1]\n",
    "IMG_SIZE = int(math.sqrt(IMG_PIX))\n",
    "TRAINING_SIZE = 60000\n",
    "TEST_SIZE = 10000\n",
    "TOTAL_SIZE = mnist.data.shape[0]\n",
    "EPOCH = 50\n",
    "BATCH_SIZE = 100\n",
    "WINDOW_SIZE = 10000/BATCH_SIZE\n",
    "\n",
    "print \"IMG_PIX: %s\\nIMG_SIZE: %s\\nTOTAL_SIZE: %s\\n\"%(IMG_PIX, IMG_SIZE, TOTAL_SIZE)\n",
    "\n",
    "def vectorize_y(Y):\n",
    "    v_y = np.zeros((Y.shape[0], max(set(Y.flatten()))+1))\n",
    "    print v_y.shape\n",
    "    for i in range(len(Y)):\n",
    "        v_y[i][Y[i]] = 1.0\n",
    "    return v_y\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    Y = np.array([Y.flatten()]).T\n",
    "    A = np.concatenate((X, Y),axis=1)\n",
    "    np.random.shuffle(A)\n",
    "    X_prime, Y_prime = np.hsplit(A, [-1])\n",
    "#     if (set(Y_prime.flatten())!=set([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0])):\n",
    "#         return shuffle(X,Y)\n",
    "    Y_prime = Y_prime.reshape(Y_prime.shape[0],)\n",
    "    return X_prime, Y_prime\n",
    "\n",
    "def prepare_data(_mnist):\n",
    "    _mnist.data = _mnist.data.astype(np.float32)\n",
    "    _mnist.data = np.multiply(_mnist.data, 1.0 / 255.0)\n",
    "    _mnist.target = _mnist.target.astype(np.uint8)\n",
    "    train_xx, test_xx = np.vsplit(_mnist.data, [TRAINING_SIZE])\n",
    "    train_yy, test_yy = np.array_split(_mnist.target, [TRAINING_SIZE])\n",
    "    return train_xx, test_xx, train_yy, test_yy\n",
    "\n",
    "print type(mnist.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training => (60000, 784) (60000,)\n",
      "testing => (10000, 784) (10000,)\n",
      "<module 'fftf' from 'fftf.pyc'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NatchaS/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:41: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "(STEP 0) loss= 2.3011 acc= 0.12 accum_acc= 0.0012 t= 0.502568006516 sec\n",
      "(STEP 10) loss= 1.5749 acc= 0.54 accum_acc= 0.0417 t= 0.567101955414 sec\n",
      "(STEP 20) loss= 1.18967 acc= 0.63 accum_acc= 0.098 t= 0.634037017822 sec\n",
      "(STEP 30) loss= 0.755785 acc= 0.73 accum_acc= 0.1688 t= 0.688642978668 sec\n",
      "(STEP 40) loss= 0.595955 acc= 0.79 accum_acc= 0.2446 t= 0.723965883255 sec\n",
      "(STEP 50) loss= 0.427661 acc= 0.86 accum_acc= 0.3211 t= 0.771932840347 sec\n",
      "(STEP 60) loss= 0.549722 acc= 0.86 accum_acc= 0.4036 t= 0.802146911621 sec\n",
      "(STEP 70) loss= 0.577971 acc= 0.79 accum_acc= 0.4865 t= 0.832627058029 sec\n",
      "(STEP 80) loss= 0.47305 acc= 0.8 accum_acc= 0.5712 t= 0.86451292038 sec\n",
      "(STEP 90) loss= 0.506539 acc= 0.87 accum_acc= 0.6589 t= 0.895432949066 sec\n",
      "(STEP 100) loss= 0.306996 acc= 0.91 accum_acc= 0.7454 t= 0.927031993866 sec\n",
      "(STEP 110) loss= 0.470392 acc= 0.85 accum_acc= 0.7947 t= 0.979745864868 sec\n",
      "(STEP 120) loss= 0.442335 acc= 0.83 accum_acc= 0.8272 t= 1.02619791031 sec\n",
      "(STEP 130) loss= 0.264254 acc= 0.92 accum_acc= 0.8476 t= 1.05589485168 sec\n",
      "(STEP 140) loss= 0.142164 acc= 0.97 accum_acc= 0.8628 t= 1.0886888504 sec\n",
      "(STEP 150) loss= 0.167678 acc= 0.92 accum_acc= 0.8772 t= 1.11807489395 sec\n",
      "(STEP 160) loss= 0.250987 acc= 0.9 accum_acc= 0.884 t= 1.14744901657 sec\n",
      "(STEP 170) loss= 0.210249 acc= 0.91 accum_acc= 0.8918 t= 1.18594384193 sec\n",
      "(STEP 180) loss= 0.164642 acc= 0.94 accum_acc= 0.8994 t= 1.2435259819 sec\n",
      "(STEP 190) loss= 0.232488 acc= 0.93 accum_acc= 0.9036 t= 1.28585386276 sec\n",
      "(STEP 200) loss= 0.206741 acc= 0.95 accum_acc= 0.9083 t= 1.35919594765 sec\n",
      "(STEP 210) loss= 0.268502 acc= 0.91 accum_acc= 0.9107 t= 1.48200392723 sec\n",
      "(STEP 220) loss= 0.154889 acc= 0.95 accum_acc= 0.9155 t= 1.55794692039 sec\n",
      "(STEP 230) loss= 0.272972 acc= 0.92 accum_acc= 0.9141 t= 1.5951499939 sec\n",
      "(STEP 240) loss= 0.140927 acc= 0.95 accum_acc= 0.9167 t= 1.62692403793 sec\n",
      "(STEP 250) loss= 0.249567 acc= 0.93 accum_acc= 0.9185 t= 1.66044592857 sec\n",
      "(STEP 260) loss= 0.185151 acc= 0.96 accum_acc= 0.9219 t= 1.69172501564 sec\n",
      "(STEP 270) loss= 0.260264 acc= 0.92 accum_acc= 0.9239 t= 1.749822855 sec\n",
      "(STEP 280) loss= 0.247267 acc= 0.91 accum_acc= 0.9237 t= 1.78254103661 sec\n",
      "(STEP 290) loss= 0.14838 acc= 0.95 accum_acc= 0.9264 t= 1.8168759346 sec\n",
      "(STEP 300) loss= 0.194705 acc= 0.93 accum_acc= 0.9275 t= 1.84750294685 sec\n",
      "(STEP 310) loss= 0.111141 acc= 0.98 accum_acc= 0.9283 t= 1.87796998024 sec\n",
      "(STEP 320) loss= 0.160544 acc= 0.95 accum_acc= 0.9292 t= 1.9186630249 sec\n",
      "(STEP 330) loss= 0.246712 acc= 0.9 accum_acc= 0.9327 t= 1.97982501984 sec\n",
      "(STEP 340) loss= 0.208933 acc= 0.94 accum_acc= 0.9337 t= 2.01529097557 sec\n",
      "(STEP 350) loss= 0.1784 acc= 0.95 accum_acc= 0.9343 t= 2.04514789581 sec\n",
      "(STEP 360) loss= 0.127904 acc= 0.96 accum_acc= 0.9363 t= 2.08362102509 sec\n",
      "(STEP 370) loss= 0.284094 acc= 0.91 accum_acc= 0.9372 t= 2.11934900284 sec\n",
      "(STEP 380) loss= 0.104863 acc= 0.98 accum_acc= 0.9395 t= 2.18459486961 sec\n",
      "(STEP 390) loss= 0.112217 acc= 0.96 accum_acc= 0.9402 t= 2.22259998322 sec\n",
      "(STEP 400) loss= 0.311538 acc= 0.89 accum_acc= 0.9397 t= 2.29087805748 sec\n",
      "(STEP 410) loss= 0.24063 acc= 0.93 accum_acc= 0.9417 t= 2.37466883659 sec\n",
      "(STEP 420) loss= 0.446926 acc= 0.88 accum_acc= 0.9417 t= 2.47693395615 sec\n",
      "(STEP 430) loss= 0.167228 acc= 0.94 accum_acc= 0.9426 t= 2.52295398712 sec\n",
      "(STEP 440) loss= 0.253203 acc= 0.92 accum_acc= 0.9433 t= 2.55344486237 sec\n",
      "(STEP 450) loss= 0.208365 acc= 0.94 accum_acc= 0.9447 t= 2.58552885056 sec\n",
      "(STEP 460) loss= 0.193262 acc= 0.94 accum_acc= 0.9444 t= 2.64626002312 sec\n",
      "(STEP 470) loss= 0.281076 acc= 0.94 accum_acc= 0.9444 t= 2.67996692657 sec\n",
      "(STEP 480) loss= 0.148317 acc= 0.95 accum_acc= 0.9445 t= 2.71149301529 sec\n",
      "(STEP 490) loss= 0.327623 acc= 0.92 accum_acc= 0.944 t= 2.7420270443 sec\n",
      "(STEP 500) loss= 0.152537 acc= 0.94 accum_acc= 0.9463 t= 2.77239704132 sec\n",
      "(STEP 510) loss= 0.0993191 acc= 0.96 accum_acc= 0.9471 t= 2.80844497681 sec\n",
      "(STEP 520) loss= 0.17082 acc= 0.96 accum_acc= 0.9486 t= 2.86763691902 sec\n",
      "(STEP 530) loss= 0.117067 acc= 0.94 accum_acc= 0.9497 t= 2.8990149498 sec\n",
      "(STEP 540) loss= 0.0753896 acc= 0.98 accum_acc= 0.9499 t= 2.9346139431 sec\n",
      "(STEP 550) loss= 0.145577 acc= 0.95 accum_acc= 0.951 t= 2.96525287628 sec\n",
      "(STEP 560) loss= 0.163041 acc= 0.92 accum_acc= 0.9512 t= 2.99945783615 sec\n",
      "(STEP 570) loss= 0.136793 acc= 0.95 accum_acc= 0.9539 t= 3.04240202904 sec\n",
      "(STEP 580) loss= 0.160225 acc= 0.96 accum_acc= 0.9544 t= 3.10025191307 sec\n",
      "(STEP 590) loss= 0.114541 acc= 0.97 accum_acc= 0.9544 t= 3.13635087013 sec\n",
      "(STEP 599) loss= 0.111503 acc= 0.97 accum_acc= 0.9539 t= 3.17262601852 sec\n",
      "EPOCH: 2\n",
      "(STEP 600) loss= 0.0921773 acc= 0.98 accum_acc= 0.9543 t= 3.54412794113 sec\n",
      "(STEP 610) loss= 0.405648 acc= 0.92 accum_acc= 0.9542 t= 3.5736989975 sec\n",
      "(STEP 620) loss= 0.126714 acc= 0.97 accum_acc= 0.9552 t= 3.60379600525 sec\n",
      "(STEP 630) loss= 0.205089 acc= 0.91 accum_acc= 0.9558 t= 3.63819384575 sec\n",
      "(STEP 640) loss= 0.329579 acc= 0.93 accum_acc= 0.9566 t= 3.66970300674 sec\n",
      "(STEP 650) loss= 0.0725389 acc= 0.98 accum_acc= 0.9566 t= 3.70430803299 sec\n",
      "(STEP 660) loss= 0.0799672 acc= 0.98 accum_acc= 0.9585 t= 3.78423595428 sec\n",
      "(STEP 670) loss= 0.0928384 acc= 0.97 accum_acc= 0.9583 t= 3.86999297142 sec\n",
      "(STEP 680) loss= 0.0741636 acc= 0.98 accum_acc= 0.9607 t= 3.9422750473 sec\n",
      "(STEP 690) loss= 0.102136 acc= 0.98 accum_acc= 0.962 t= 4.01111292839 sec\n",
      "(STEP 700) loss= 0.108122 acc= 0.96 accum_acc= 0.9634 t= 4.06701397896 sec\n",
      "(STEP 710) loss= 0.221762 acc= 0.93 accum_acc= 0.9638 t= 4.09715986252 sec\n",
      "(STEP 720) loss= 0.149199 acc= 0.94 accum_acc= 0.9634 t= 4.12831687927 sec\n",
      "(STEP 730) loss= 0.0299138 acc= 0.99 accum_acc= 0.9648 t= 4.15831804276 sec\n",
      "(STEP 740) loss= 0.196115 acc= 0.91 accum_acc= 0.965 t= 4.18991184235 sec\n",
      "(STEP 750) loss= 0.135796 acc= 0.93 accum_acc= 0.964 t= 4.2312400341 sec\n",
      "(STEP 760) loss= 0.0669302 acc= 0.98 accum_acc= 0.9642 t= 4.27777791023 sec\n",
      "(STEP 770) loss= 0.15893 acc= 0.96 accum_acc= 0.9642 t= 4.31530284882 sec\n",
      "(STEP 780) loss= 0.170995 acc= 0.96 accum_acc= 0.9622 t= 4.34540486336 sec\n",
      "(STEP 790) loss= 0.0785555 acc= 0.97 accum_acc= 0.9626 t= 4.37636494637 sec\n",
      "(STEP 800) loss= 0.137567 acc= 0.95 accum_acc= 0.9637 t= 4.40795898438 sec\n",
      "(STEP 810) loss= 0.141 acc= 0.97 accum_acc= 0.9638 t= 4.44066095352 sec\n",
      "(STEP 820) loss= 0.0917199 acc= 0.98 accum_acc= 0.9639 t= 4.49355983734 sec\n",
      "(STEP 830) loss= 0.0653039 acc= 0.98 accum_acc= 0.9637 t= 4.53092384338 sec\n",
      "(STEP 840) loss= 0.102847 acc= 0.96 accum_acc= 0.9645 t= 4.56673693657 sec\n",
      "(STEP 850) loss= 0.166267 acc= 0.94 accum_acc= 0.9656 t= 4.59930992126 sec\n",
      "(STEP 860) loss= 0.048575 acc= 1.0 accum_acc= 0.9662 t= 4.63726782799 sec\n",
      "(STEP 870) loss= 0.078079 acc= 0.97 accum_acc= 0.9662 t= 4.6863360405 sec\n",
      "(STEP 880) loss= 0.0937165 acc= 0.98 accum_acc= 0.9673 t= 4.73509383202 sec\n",
      "(STEP 890) loss= 0.143938 acc= 0.94 accum_acc= 0.9677 t= 4.76821494102 sec\n",
      "(STEP 900) loss= 0.179062 acc= 0.92 accum_acc= 0.9659 t= 4.79934000969 sec\n",
      "(STEP 910) loss= 0.129871 acc= 0.97 accum_acc= 0.9666 t= 4.84441685677 sec\n",
      "(STEP 920) loss= 0.0260588 acc= 1.0 accum_acc= 0.9666 t= 4.94761300087 sec\n",
      "(STEP 930) loss= 0.0566569 acc= 0.98 accum_acc= 0.9657 t= 5.00775885582 sec\n",
      "(STEP 940) loss= 0.0978484 acc= 0.96 accum_acc= 0.9648 t= 5.0387070179 sec\n",
      "(STEP 950) loss= 0.134728 acc= 0.95 accum_acc= 0.9652 t= 5.0730779171 sec\n",
      "(STEP 960) loss= 0.254936 acc= 0.93 accum_acc= 0.9646 t= 5.11703801155 sec\n",
      "(STEP 970) loss= 0.0806308 acc= 0.98 accum_acc= 0.9642 t= 5.15764403343 sec\n",
      "(STEP 980) loss= 0.134443 acc= 0.94 accum_acc= 0.9643 t= 5.20153784752 sec\n",
      "(STEP 990) loss= 0.132953 acc= 0.95 accum_acc= 0.9638 t= 5.2422208786 sec\n",
      "(STEP 1000) loss= 0.147186 acc= 0.98 accum_acc= 0.9645 t= 5.27256202698 sec\n",
      "(STEP 1010) loss= 0.102353 acc= 0.97 accum_acc= 0.9649 t= 5.30958390236 sec\n",
      "(STEP 1020) loss= 0.0644235 acc= 0.96 accum_acc= 0.9644 t= 5.34311699867 sec\n",
      "(STEP 1030) loss= 0.0459819 acc= 0.99 accum_acc= 0.9644 t= 5.38603901863 sec\n",
      "(STEP 1040) loss= 0.0719285 acc= 0.98 accum_acc= 0.9655 t= 5.4245569706 sec\n",
      "(STEP 1050) loss= 0.0683255 acc= 0.99 accum_acc= 0.9661 t= 5.45658397675 sec\n",
      "(STEP 1060) loss= 0.152268 acc= 0.94 accum_acc= 0.9661 t= 5.48834490776 sec\n",
      "(STEP 1070) loss= 0.133581 acc= 0.96 accum_acc= 0.9667 t= 5.52658891678 sec\n",
      "(STEP 1080) loss= 0.0975467 acc= 0.99 accum_acc= 0.9671 t= 5.56267786026 sec\n",
      "(STEP 1090) loss= 0.121266 acc= 0.96 accum_acc= 0.9675 t= 5.59841799736 sec\n",
      "(STEP 1100) loss= 0.0729795 acc= 0.98 accum_acc= 0.9674 t= 5.64000892639 sec\n",
      "(STEP 1110) loss= 0.109951 acc= 0.96 accum_acc= 0.9671 t= 5.67518591881 sec\n",
      "(STEP 1120) loss= 0.0455752 acc= 0.99 accum_acc= 0.9685 t= 5.70925593376 sec\n",
      "(STEP 1130) loss= 0.14732 acc= 0.97 accum_acc= 0.9679 t= 5.75271892548 sec\n",
      "(STEP 1140) loss= 0.164231 acc= 0.95 accum_acc= 0.9678 t= 5.79077601433 sec\n",
      "(STEP 1150) loss= 0.0905089 acc= 0.98 accum_acc= 0.9675 t= 5.82438588142 sec\n",
      "(STEP 1160) loss= 0.14796 acc= 0.97 accum_acc= 0.9677 t= 5.8656308651 sec\n",
      "(STEP 1170) loss= 0.105374 acc= 0.97 accum_acc= 0.9689 t= 5.89577698708 sec\n",
      "(STEP 1180) loss= 0.0593476 acc= 0.98 accum_acc= 0.9686 t= 5.92826199532 sec\n",
      "(STEP 1190) loss= 0.1691 acc= 0.95 accum_acc= 0.968 t= 5.9593000412 sec\n",
      "(STEP 1199) loss= 0.058442 acc= 0.99 accum_acc= 0.9687 t= 5.98612689972 sec\n",
      "EPOCH: 3\n",
      "(STEP 1200) loss= 0.039145 acc= 1.0 accum_acc= 0.9689 t= 6.40795683861 sec\n",
      "(STEP 1210) loss= 0.0850261 acc= 0.97 accum_acc= 0.9692 t= 6.44015693665 sec\n",
      "(STEP 1220) loss= 0.0260997 acc= 1.0 accum_acc= 0.9696 t= 6.47089385986 sec\n",
      "-----------------\n",
      "(STEP 1222) loss= 0.0721293 acc= 0.98 accum_acc= 0.9702 t= 6.47714805603 sec\n",
      "\tTEST ACCURACY= 0.9698\n"
     ]
    }
   ],
   "source": [
    "import fftf\n",
    "import ffbn\n",
    "\n",
    "HIDDEN_1 = 100\n",
    "HIDDEN_2 = 100\n",
    "ALPHA = 0.5\n",
    "USING_BATCH_NORM = False\n",
    "if (USING_BATCH_NORM):\n",
    "    import ffbn as ff\n",
    "else: \n",
    "    import fftf as ff\n",
    "\n",
    "train_x, test_x, train_y, test_y = prepare_data(mnist)\n",
    "print \"training =>\", train_x.shape, train_y.shape\n",
    "print \"testing =>\", test_x.shape, test_y.shape\n",
    "print ff\n",
    "\n",
    "with tf.Graph().as_default():      \n",
    "    img_input = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_PIX])\n",
    "    label_input = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "    model = None\n",
    "    if (USING_BATCH_NORM):\n",
    "        model = ff.inference(img_input, HIDDEN_1, HIDDEN_2, batch_norm_ver=None)\n",
    "    else:\n",
    "        model = ff.inference(img_input, HIDDEN_1, HIDDEN_2)\n",
    "    loss_f = ff.loss(model, label_input)\n",
    "    training_model = ff.training(loss_f, ALPHA)\n",
    "    eval_f = ff.evaluation(model, label_input)\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    loss_arr = []\n",
    "    eval_arr = []\n",
    "    moving_done = False\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        t0 = time.time()\n",
    "        moving_idx = 0\n",
    "        moving_avg = np.zeros(WINDOW_SIZE)\n",
    "        step = 0\n",
    "        for i in xrange(EPOCH):\n",
    "            print \"EPOCH:\", i+1\n",
    "            acc_sum = 0\n",
    "            train_x, train_y = shuffle(train_x, train_y)\n",
    "            for j in xrange(0, TRAINING_SIZE, BATCH_SIZE):\n",
    "                train_x_batch = train_x[j:j+BATCH_SIZE]\n",
    "                train_y_batch = train_y[j:j+BATCH_SIZE]\n",
    "                _, loss, correct = sess.run([training_model, loss_f, eval_f], feed_dict={\n",
    "                    img_input: train_x_batch,\n",
    "                    label_input: train_y_batch\n",
    "                })\n",
    "                \n",
    "                moving_avg[moving_idx] = correct/len(train_x_batch)\n",
    "                moving_idx += 1\n",
    "                if (moving_idx >= WINDOW_SIZE):\n",
    "                    moving_idx = 0\n",
    "                    moving_done = True\n",
    "                \n",
    "                wind_avg= moving_avg.sum()/WINDOW_SIZE\n",
    "                \n",
    "                if(j%1000)==0 or j+BATCH_SIZE == TRAINING_SIZE:\n",
    "                    print \"(STEP %s) loss= %s acc= %s accum_acc= %s t= %s sec\"%(step, loss, correct/BATCH_SIZE, wind_avg, time.time()-t0)\n",
    "                \n",
    "                if (moving_done and wind_avg >= 0.97):\n",
    "                    print \"-----------------\"\n",
    "                    print \"(STEP %s) loss= %s acc= %s accum_acc= %s t= %s sec\"%(step, loss, correct/BATCH_SIZE, wind_avg, time.time()-t0)\n",
    "                    done = True\n",
    "                    break;\n",
    "                step+=1\n",
    "                    \n",
    "            if (done):\n",
    "                break;\n",
    "        num_correct = 0\n",
    "        for k in xrange(0, TEST_SIZE, BATCH_SIZE):\n",
    "            eval_x, eval_y = test_x[k:k+BATCH_SIZE], test_y[k:k+BATCH_SIZE]\n",
    "            if (len(eval_x)!=BATCH_SIZE): \n",
    "                break\n",
    "            correct = sess.run(eval_f, feed_dict={\n",
    "                img_input: eval_x,\n",
    "                label_input: eval_y\n",
    "            })\n",
    "            num_correct += correct\n",
    "#                 print num_correct, correct\n",
    "        print \"\\tTEST ACCURACY= %s\"%(num_correct/TEST_SIZE)\n",
    "                    \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "# ds = input_data.read_data_sets(\"data\", validation_size=0)\n",
    "\n",
    "# print ds.train.epochs_completed\n",
    "# print ds.train.images.shape\n",
    "\n",
    "# print train_y[0]==ds.train.labels[1]\n",
    "# print set(ds.train.images[1]) == set(train_x[0])\n",
    "\n",
    "# plt.subplot(3,3,1)\n",
    "# plt.imshow(train_x[0].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,2)\n",
    "# plt.imshow(train_x[1].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,3)\n",
    "# plt.imshow(train_x[2].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,4)\n",
    "# plt.imshow(train_x[3].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,5)\n",
    "# plt.imshow(train_x[4].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,6)\n",
    "# plt.imshow(train_x[5].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,7)\n",
    "# plt.imshow(train_x[6].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,8)\n",
    "# plt.imshow(train_x[7].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,9)\n",
    "# plt.imshow(train_x[8].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "\n",
    "# # plt.subplot(1,2,2)\n",
    "# # plt.imshow(ds.train.images[3].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# # print train_y[2]\n",
    "# # print ds.train.labels[3]\n",
    "\n",
    "# print type(ds.train.labels[0])\n",
    "# print train_y[:10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
