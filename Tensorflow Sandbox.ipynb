{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NatchaS/anaconda/envs/tensorflow/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_PIX: 784\n",
      "IMG_SIZE: 28\n",
      "TOTAL_SIZE: 70000\n",
      "\n",
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import fetch_mldata\n",
    "import sklearn.datasets as sk_data\n",
    "import math\n",
    "import time\n",
    "\n",
    "mnist = fetch_mldata('MNIST original', data_home=sk_data.get_data_home())\n",
    "\n",
    "IMG_PIX = mnist.data.shape[1]\n",
    "IMG_SIZE = int(math.sqrt(IMG_PIX))\n",
    "TRAINING_SIZE = 60000\n",
    "TEST_SIZE = 10000\n",
    "TOTAL_SIZE = mnist.data.shape[0]\n",
    "EPOCH = 50\n",
    "BATCH_SIZE = 100\n",
    "WINDOW_SIZE = 10000/BATCH_SIZE\n",
    "\n",
    "print \"IMG_PIX: %s\\nIMG_SIZE: %s\\nTOTAL_SIZE: %s\\n\"%(IMG_PIX, IMG_SIZE, TOTAL_SIZE)\n",
    "\n",
    "def vectorize_y(Y):\n",
    "    v_y = np.zeros((Y.shape[0], max(set(Y.flatten()))+1))\n",
    "    print v_y.shape\n",
    "    for i in range(len(Y)):\n",
    "        v_y[i][Y[i]] = 1.0\n",
    "    return v_y\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    Y = np.array([Y.flatten()]).T\n",
    "    A = np.concatenate((X, Y),axis=1)\n",
    "    np.random.shuffle(A)\n",
    "    X_prime, Y_prime = np.hsplit(A, [-1])\n",
    "#     if (set(Y_prime.flatten())!=set([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0])):\n",
    "#         return shuffle(X,Y)\n",
    "    Y_prime = Y_prime.reshape(Y_prime.shape[0],)\n",
    "    return X_prime, Y_prime\n",
    "\n",
    "def prepare_data(_mnist):\n",
    "    _mnist.data = _mnist.data.astype(np.float32)\n",
    "    _mnist.data = np.multiply(_mnist.data, 1.0 / 255.0)\n",
    "    _mnist.target = _mnist.target.astype(np.uint8)\n",
    "    train_xx, test_xx = np.vsplit(_mnist.data, [TRAINING_SIZE])\n",
    "    train_yy, test_yy = np.array_split(_mnist.target, [TRAINING_SIZE])\n",
    "    return train_xx, test_xx, train_yy, test_yy\n",
    "\n",
    "print type(mnist.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training => (60000, 784) (60000,)\n",
      "testing => (10000, 784) (10000,)\n",
      "<module 'ffbn' from 'ffbn.pyc'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NatchaS/anaconda/envs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:41: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "(STEP 0) loss= 2.29058 acc= 0.13 accum_acc= 0.0013 t= 0.634057998657 sec\n",
      "(STEP 10) loss= 1.82047 acc= 0.74 accum_acc= 0.0564 t= 0.699970006943 sec\n",
      "(STEP 20) loss= 1.87464 acc= 0.6 accum_acc= 0.1212 t= 0.756870985031 sec\n",
      "(STEP 30) loss= 1.78725 acc= 0.73 accum_acc= 0.1964 t= 0.812235116959 sec\n",
      "(STEP 40) loss= 1.72989 acc= 0.77 accum_acc= 0.2756 t= 0.877447128296 sec\n",
      "(STEP 50) loss= 1.67876 acc= 0.87 accum_acc= 0.3625 t= 0.938745975494 sec\n",
      "(STEP 60) loss= 1.65404 acc= 0.83 accum_acc= 0.4484 t= 0.997604131699 sec\n",
      "(STEP 70) loss= 1.5995 acc= 0.88 accum_acc= 0.5365 t= 1.05627298355 sec\n",
      "(STEP 80) loss= 1.57212 acc= 0.94 accum_acc= 0.6276 t= 1.13361215591 sec\n",
      "(STEP 90) loss= 1.60128 acc= 0.9 accum_acc= 0.7184 t= 1.21359705925 sec\n",
      "(STEP 100) loss= 1.60873 acc= 0.88 accum_acc= 0.8071 t= 1.3574359417 sec\n",
      "(STEP 110) loss= 1.59398 acc= 0.89 accum_acc= 0.8424 t= 1.49846696854 sec\n",
      "(STEP 120) loss= 1.54852 acc= 0.93 accum_acc= 0.8694 t= 1.55570602417 sec\n",
      "(STEP 130) loss= 1.58136 acc= 0.9 accum_acc= 0.8859 t= 1.63185405731 sec\n",
      "(STEP 140) loss= 1.56406 acc= 0.93 accum_acc= 0.8995 t= 1.68506097794 sec\n",
      "(STEP 150) loss= 1.55024 acc= 0.92 accum_acc= 0.9041 t= 1.74263310432 sec\n",
      "(STEP 160) loss= 1.51769 acc= 0.98 accum_acc= 0.9104 t= 1.7983520031 sec\n",
      "(STEP 170) loss= 1.55163 acc= 0.94 accum_acc= 0.9135 t= 1.86022806168 sec\n",
      "(STEP 180) loss= 1.59377 acc= 0.89 accum_acc= 0.9136 t= 1.9372420311 sec\n",
      "(STEP 190) loss= 1.55057 acc= 0.92 accum_acc= 0.9158 t= 2.01096796989 sec\n",
      "(STEP 200) loss= 1.58169 acc= 0.9 accum_acc= 0.9193 t= 2.08006095886 sec\n",
      "(STEP 210) loss= 1.52954 acc= 0.94 accum_acc= 0.9227 t= 2.16912508011 sec\n",
      "(STEP 220) loss= 1.50177 acc= 0.98 accum_acc= 0.9245 t= 2.30914211273 sec\n",
      "(STEP 230) loss= 1.52914 acc= 0.95 accum_acc= 0.9249 t= 2.44923996925 sec\n",
      "(STEP 240) loss= 1.51749 acc= 0.96 accum_acc= 0.9253 t= 2.51098895073 sec\n",
      "(STEP 250) loss= 1.53728 acc= 0.93 accum_acc= 0.928 t= 2.59132814407 sec\n",
      "(STEP 260) loss= 1.54789 acc= 0.93 accum_acc= 0.928 t= 2.65001296997 sec\n",
      "(STEP 270) loss= 1.5316 acc= 0.94 accum_acc= 0.9295 t= 2.72395610809 sec\n",
      "(STEP 280) loss= 1.57265 acc= 0.89 accum_acc= 0.9309 t= 2.81380200386 sec\n",
      "(STEP 290) loss= 1.54554 acc= 0.93 accum_acc= 0.933 t= 2.8646569252 sec\n",
      "(STEP 300) loss= 1.56175 acc= 0.93 accum_acc= 0.9347 t= 2.93356108665 sec\n",
      "(STEP 310) loss= 1.55637 acc= 0.92 accum_acc= 0.9352 t= 3.02784609795 sec\n",
      "(STEP 320) loss= 1.51371 acc= 0.97 accum_acc= 0.9372 t= 3.097260952 sec\n",
      "(STEP 330) loss= 1.54577 acc= 0.93 accum_acc= 0.9384 t= 3.1659309864 sec\n",
      "(STEP 340) loss= 1.49802 acc= 0.97 accum_acc= 0.9374 t= 3.23952698708 sec\n",
      "(STEP 350) loss= 1.53923 acc= 0.91 accum_acc= 0.9378 t= 3.31637406349 sec\n",
      "(STEP 360) loss= 1.49748 acc= 0.98 accum_acc= 0.9409 t= 3.38490104675 sec\n",
      "(STEP 370) loss= 1.55834 acc= 0.91 accum_acc= 0.9428 t= 3.46781206131 sec\n",
      "(STEP 380) loss= 1.53379 acc= 0.93 accum_acc= 0.9454 t= 3.52017593384 sec\n",
      "(STEP 390) loss= 1.52286 acc= 0.96 accum_acc= 0.9453 t= 3.57513999939 sec\n",
      "(STEP 400) loss= 1.4867 acc= 0.99 accum_acc= 0.9453 t= 3.65487313271 sec\n",
      "(STEP 410) loss= 1.51145 acc= 0.96 accum_acc= 0.9467 t= 3.7152569294 sec\n",
      "(STEP 420) loss= 1.51242 acc= 0.97 accum_acc= 0.9466 t= 3.7786860466 sec\n",
      "(STEP 430) loss= 1.49824 acc= 0.98 accum_acc= 0.948 t= 3.837018013 sec\n",
      "(STEP 440) loss= 1.51741 acc= 0.95 accum_acc= 0.9496 t= 3.97915315628 sec\n",
      "(STEP 450) loss= 1.51352 acc= 0.94 accum_acc= 0.9492 t= 4.10280394554 sec\n",
      "(STEP 460) loss= 1.51335 acc= 0.97 accum_acc= 0.9488 t= 4.18171811104 sec\n",
      "(STEP 470) loss= 1.51873 acc= 0.95 accum_acc= 0.9492 t= 4.32366204262 sec\n",
      "(STEP 480) loss= 1.51162 acc= 0.97 accum_acc= 0.9498 t= 4.43428397179 sec\n",
      "(STEP 490) loss= 1.51395 acc= 0.97 accum_acc= 0.9496 t= 4.50108194351 sec\n",
      "(STEP 500) loss= 1.52613 acc= 0.95 accum_acc= 0.9491 t= 4.5879149437 sec\n",
      "(STEP 510) loss= 1.5218 acc= 0.94 accum_acc= 0.9497 t= 4.64018106461 sec\n",
      "(STEP 520) loss= 1.53478 acc= 0.92 accum_acc= 0.9489 t= 4.69334411621 sec\n",
      "(STEP 530) loss= 1.51703 acc= 0.95 accum_acc= 0.9499 t= 4.75527906418 sec\n",
      "(STEP 540) loss= 1.53594 acc= 0.92 accum_acc= 0.9519 t= 4.81708407402 sec\n",
      "(STEP 550) loss= 1.5505 acc= 0.91 accum_acc= 0.9528 t= 4.90304493904 sec\n",
      "(STEP 560) loss= 1.51462 acc= 0.97 accum_acc= 0.9533 t= 4.97109007835 sec\n",
      "(STEP 570) loss= 1.51431 acc= 0.96 accum_acc= 0.9533 t= 5.04434800148 sec\n",
      "(STEP 580) loss= 1.53702 acc= 0.93 accum_acc= 0.9534 t= 5.13616299629 sec\n",
      "(STEP 590) loss= 1.5466 acc= 0.93 accum_acc= 0.9533 t= 5.25358915329 sec\n",
      "(STEP 599) loss= 1.55373 acc= 0.91 accum_acc= 0.9543 t= 5.36021614075 sec\n",
      "EPOCH: 2\n",
      "(STEP 600) loss= 1.51881 acc= 0.96 accum_acc= 0.9544 t= 5.82936811447 sec\n",
      "(STEP 610) loss= 1.4883 acc= 0.97 accum_acc= 0.9535 t= 5.89527606964 sec\n",
      "(STEP 620) loss= 1.49886 acc= 0.97 accum_acc= 0.9543 t= 5.96919894218 sec\n",
      "(STEP 630) loss= 1.4924 acc= 0.99 accum_acc= 0.955 t= 6.03646612167 sec\n",
      "(STEP 640) loss= 1.50851 acc= 0.96 accum_acc= 0.9553 t= 6.12367796898 sec\n",
      "(STEP 650) loss= 1.49854 acc= 0.97 accum_acc= 0.9559 t= 6.18302798271 sec\n",
      "(STEP 660) loss= 1.53169 acc= 0.94 accum_acc= 0.956 t= 6.32986807823 sec\n",
      "(STEP 670) loss= 1.48665 acc= 0.98 accum_acc= 0.9574 t= 6.38719296455 sec\n",
      "(STEP 680) loss= 1.50182 acc= 0.97 accum_acc= 0.9581 t= 6.44845199585 sec\n",
      "(STEP 690) loss= 1.50498 acc= 0.97 accum_acc= 0.9598 t= 6.5007390976 sec\n",
      "(STEP 700) loss= 1.51206 acc= 0.96 accum_acc= 0.9601 t= 6.56145501137 sec\n",
      "(STEP 710) loss= 1.50155 acc= 0.97 accum_acc= 0.9604 t= 6.65828609467 sec\n",
      "(STEP 720) loss= 1.4835 acc= 0.99 accum_acc= 0.9609 t= 6.73788714409 sec\n",
      "(STEP 730) loss= 1.5132 acc= 0.96 accum_acc= 0.96 t= 6.84926509857 sec\n",
      "(STEP 740) loss= 1.50242 acc= 0.96 accum_acc= 0.9593 t= 6.99120807648 sec\n",
      "(STEP 750) loss= 1.48462 acc= 0.98 accum_acc= 0.9588 t= 7.11962103844 sec\n",
      "(STEP 760) loss= 1.5118 acc= 0.97 accum_acc= 0.9595 t= 7.19539904594 sec\n",
      "(STEP 770) loss= 1.50904 acc= 0.96 accum_acc= 0.9591 t= 7.24978303909 sec\n",
      "(STEP 780) loss= 1.48152 acc= 0.99 accum_acc= 0.9584 t= 7.32904410362 sec\n",
      "(STEP 790) loss= 1.50462 acc= 0.97 accum_acc= 0.959 t= 7.42936110497 sec\n",
      "(STEP 800) loss= 1.49155 acc= 0.98 accum_acc= 0.9598 t= 7.48465108871 sec\n",
      "(STEP 810) loss= 1.5098 acc= 0.96 accum_acc= 0.9595 t= 7.54171204567 sec\n",
      "(STEP 820) loss= 1.48144 acc= 0.99 accum_acc= 0.9586 t= 7.63110208511 sec\n",
      "(STEP 830) loss= 1.48535 acc= 0.98 accum_acc= 0.9599 t= 7.69022607803 sec\n",
      "(STEP 840) loss= 1.50918 acc= 0.97 accum_acc= 0.9616 t= 7.7637360096 sec\n",
      "(STEP 850) loss= 1.47592 acc= 1.0 accum_acc= 0.963 t= 7.86749315262 sec\n",
      "(STEP 860) loss= 1.52833 acc= 0.94 accum_acc= 0.9633 t= 7.92398405075 sec\n",
      "(STEP 870) loss= 1.49201 acc= 0.99 accum_acc= 0.9638 t= 7.97987914085 sec\n",
      "(STEP 880) loss= 1.5225 acc= 0.95 accum_acc= 0.9654 t= 8.06414604187 sec\n",
      "(STEP 890) loss= 1.49702 acc= 0.97 accum_acc= 0.9648 t= 8.12310695648 sec\n",
      "(STEP 900) loss= 1.47681 acc= 0.99 accum_acc= 0.9644 t= 8.18331813812 sec\n",
      "(STEP 910) loss= 1.5012 acc= 0.97 accum_acc= 0.9647 t= 8.28200793266 sec\n",
      "(STEP 920) loss= 1.51249 acc= 0.95 accum_acc= 0.9657 t= 8.45043110847 sec\n",
      "(STEP 930) loss= 1.52621 acc= 0.93 accum_acc= 0.9661 t= 8.5604031086 sec\n",
      "(STEP 940) loss= 1.49339 acc= 0.97 accum_acc= 0.9659 t= 8.62610507011 sec\n",
      "(STEP 950) loss= 1.52497 acc= 0.93 accum_acc= 0.9649 t= 8.70279407501 sec\n",
      "(STEP 960) loss= 1.50812 acc= 0.96 accum_acc= 0.9657 t= 8.78225207329 sec\n",
      "(STEP 970) loss= 1.54761 acc= 0.91 accum_acc= 0.9652 t= 8.84317302704 sec\n",
      "(STEP 980) loss= 1.49625 acc= 0.96 accum_acc= 0.9642 t= 8.89495897293 sec\n",
      "(STEP 990) loss= 1.52218 acc= 0.93 accum_acc= 0.964 t= 8.96156311035 sec\n",
      "(STEP 1000) loss= 1.49542 acc= 0.97 accum_acc= 0.9644 t= 9.05755209923 sec\n",
      "(STEP 1010) loss= 1.49354 acc= 0.97 accum_acc= 0.9652 t= 9.1120569706 sec\n",
      "(STEP 1020) loss= 1.49679 acc= 0.97 accum_acc= 0.9662 t= 9.17908406258 sec\n",
      "(STEP 1030) loss= 1.5153 acc= 0.95 accum_acc= 0.9655 t= 9.2374830246 sec\n",
      "(STEP 1040) loss= 1.48114 acc= 0.99 accum_acc= 0.9653 t= 9.29425096512 sec\n",
      "(STEP 1050) loss= 1.49638 acc= 0.96 accum_acc= 0.9656 t= 9.35654592514 sec\n",
      "(STEP 1060) loss= 1.52865 acc= 0.93 accum_acc= 0.964 t= 9.41763997078 sec\n",
      "(STEP 1070) loss= 1.51528 acc= 0.95 accum_acc= 0.964 t= 9.48081994057 sec\n",
      "(STEP 1080) loss= 1.49973 acc= 0.97 accum_acc= 0.9642 t= 9.55242705345 sec\n",
      "(STEP 1090) loss= 1.54444 acc= 0.92 accum_acc= 0.9633 t= 9.61244797707 sec\n",
      "(STEP 1100) loss= 1.52354 acc= 0.94 accum_acc= 0.9631 t= 9.68223595619 sec\n",
      "(STEP 1110) loss= 1.53313 acc= 0.93 accum_acc= 0.9625 t= 9.7497241497 sec\n",
      "(STEP 1120) loss= 1.50901 acc= 0.95 accum_acc= 0.9624 t= 9.81140303612 sec\n",
      "(STEP 1130) loss= 1.50607 acc= 0.97 accum_acc= 0.9627 t= 9.87290000916 sec\n",
      "(STEP 1140) loss= 1.48961 acc= 0.97 accum_acc= 0.9635 t= 9.94130396843 sec\n",
      "(STEP 1150) loss= 1.52921 acc= 0.94 accum_acc= 0.9639 t= 9.99720001221 sec\n",
      "(STEP 1160) loss= 1.4995 acc= 0.98 accum_acc= 0.965 t= 10.0900430679 sec\n",
      "(STEP 1170) loss= 1.50855 acc= 0.96 accum_acc= 0.9653 t= 10.2071499825 sec\n",
      "(STEP 1180) loss= 1.49216 acc= 0.98 accum_acc= 0.9653 t= 10.3556549549 sec\n",
      "(STEP 1190) loss= 1.48038 acc= 0.99 accum_acc= 0.9668 t= 10.4438481331 sec\n",
      "(STEP 1199) loss= 1.49044 acc= 0.98 accum_acc= 0.9658 t= 10.5025379658 sec\n",
      "EPOCH: 3\n",
      "(STEP 1200) loss= 1.4973 acc= 0.97 accum_acc= 0.9661 t= 10.8082299232 sec\n",
      "(STEP 1210) loss= 1.51732 acc= 0.95 accum_acc= 0.9675 t= 10.8899049759 sec\n",
      "(STEP 1220) loss= 1.49302 acc= 0.97 accum_acc= 0.967 t= 11.0016980171 sec\n",
      "(STEP 1230) loss= 1.48932 acc= 0.98 accum_acc= 0.9669 t= 11.1069469452 sec\n",
      "(STEP 1240) loss= 1.50289 acc= 0.97 accum_acc= 0.9666 t= 11.1850049496 sec\n",
      "(STEP 1250) loss= 1.47368 acc= 0.99 accum_acc= 0.9677 t= 11.2772741318 sec\n",
      "(STEP 1260) loss= 1.48012 acc= 0.99 accum_acc= 0.9682 t= 11.3847689629 sec\n",
      "(STEP 1270) loss= 1.49342 acc= 0.97 accum_acc= 0.9687 t= 11.4484550953 sec\n",
      "(STEP 1280) loss= 1.50282 acc= 0.96 accum_acc= 0.9683 t= 11.5040900707 sec\n",
      "(STEP 1290) loss= 1.50694 acc= 0.96 accum_acc= 0.9686 t= 11.5619909763 sec\n",
      "(STEP 1300) loss= 1.47629 acc= 1.0 accum_acc= 0.9695 t= 11.6334810257 sec\n",
      "(STEP 1310) loss= 1.51217 acc= 0.96 accum_acc= 0.9687 t= 11.7400789261 sec\n",
      "(STEP 1320) loss= 1.48532 acc= 0.99 accum_acc= 0.9693 t= 11.8393390179 sec\n",
      "(STEP 1330) loss= 1.47427 acc= 0.99 accum_acc= 0.9697 t= 11.9089770317 sec\n",
      "-----------------\n",
      "(STEP 1333) loss= 1.48523 acc= 0.98 accum_acc= 0.9701 t= 11.9253189564 sec\n",
      "\tTEST ACCURACY= 0.1557\n"
     ]
    }
   ],
   "source": [
    "import fftf\n",
    "import ffbn\n",
    "\n",
    "HIDDEN_1 = 100\n",
    "HIDDEN_2 = 100\n",
    "ALPHA = 0.55\n",
    "USING_BATCH_NORM = True\n",
    "if (USING_BATCH_NORM):\n",
    "    import ffbn as ff\n",
    "else: \n",
    "    import fftf as ff\n",
    "\n",
    "train_x, test_x, train_y, test_y = prepare_data(mnist)\n",
    "print \"training =>\", train_x.shape, train_y.shape\n",
    "print \"testing =>\", test_x.shape, test_y.shape\n",
    "print ff\n",
    "\n",
    "with tf.Graph().as_default():      \n",
    "    img_input = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_PIX])\n",
    "    label_input = tf.placeholder(tf.int32, shape=[BATCH_SIZE])\n",
    "    model = None\n",
    "    if (USING_BATCH_NORM):\n",
    "        model = ff.inference(img_input, HIDDEN_1, HIDDEN_2, batch_norm_ver=None)\n",
    "    else:\n",
    "        model = ff.inference(img_input, HIDDEN_1, HIDDEN_2)\n",
    "    loss_f = ff.loss(model, label_input)\n",
    "    training_model = ff.training(loss_f, ALPHA)\n",
    "    eval_f = ff.evaluation(model, label_input)\n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    loss_arr = []\n",
    "    eval_arr = []\n",
    "    moving_done = False\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        t0 = time.time()\n",
    "        moving_idx = 0\n",
    "        moving_avg = np.zeros(WINDOW_SIZE)\n",
    "        step = 0\n",
    "        for i in xrange(EPOCH):\n",
    "            print \"EPOCH:\", i+1\n",
    "            acc_sum = 0\n",
    "            train_x, train_y = shuffle(train_x, train_y)\n",
    "            for j in xrange(0, TRAINING_SIZE, BATCH_SIZE):\n",
    "                train_x_batch = train_x[j:j+BATCH_SIZE]\n",
    "                train_y_batch = train_y[j:j+BATCH_SIZE]\n",
    "                _, loss, correct = sess.run([training_model, loss_f, eval_f], feed_dict={\n",
    "                    img_input: train_x_batch,\n",
    "                    label_input: train_y_batch\n",
    "                })\n",
    "                \n",
    "                moving_avg[moving_idx] = correct/BATCH_SIZE\n",
    "                moving_idx += 1\n",
    "                if (moving_idx >= WINDOW_SIZE):\n",
    "                    moving_idx = 0\n",
    "                    moving_done = True\n",
    "                \n",
    "                wind_avg= moving_avg.sum()/WINDOW_SIZE\n",
    "                \n",
    "                if(j%1000)==0 or j+BATCH_SIZE == TRAINING_SIZE:\n",
    "                    print \"(STEP %s) loss= %s acc= %s accum_acc= %s t= %s sec\"%(step, loss, correct/BATCH_SIZE, wind_avg, time.time()-t0)\n",
    "                \n",
    "                if (moving_done and wind_avg >= 0.97):\n",
    "                    print \"-----------------\"\n",
    "                    print \"(STEP %s) loss= %s acc= %s accum_acc= %s t= %s sec\"%(step, loss, correct/BATCH_SIZE, wind_avg, time.time()-t0)\n",
    "                    done = True\n",
    "                    break;\n",
    "                step+=1\n",
    "                    \n",
    "            if (done):\n",
    "                break;\n",
    "        num_correct = 0\n",
    "        for k in xrange(0, TEST_SIZE, BATCH_SIZE):\n",
    "            eval_x, eval_y = test_x[k:k+BATCH_SIZE], test_y[k:k+BATCH_SIZE]\n",
    "            if (len(eval_x)!=BATCH_SIZE): \n",
    "                break\n",
    "            correct = sess.run(eval_f, feed_dict={\n",
    "                img_input: eval_x,\n",
    "                label_input: eval_y\n",
    "            })\n",
    "            num_correct += correct\n",
    "#                 print num_correct, correct\n",
    "        print \"\\tTEST ACCURACY= %s\"%(num_correct/TEST_SIZE)\n",
    "                    \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "# ds = input_data.read_data_sets(\"data\", validation_size=0)\n",
    "\n",
    "# print ds.train.epochs_completed\n",
    "# print ds.train.images.shape\n",
    "\n",
    "# print train_y[0]==ds.train.labels[1]\n",
    "# print set(ds.train.images[1]) == set(train_x[0])\n",
    "\n",
    "# plt.subplot(3,3,1)\n",
    "# plt.imshow(train_x[0].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,2)\n",
    "# plt.imshow(train_x[1].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,3)\n",
    "# plt.imshow(train_x[2].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,4)\n",
    "# plt.imshow(train_x[3].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,5)\n",
    "# plt.imshow(train_x[4].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,6)\n",
    "# plt.imshow(train_x[5].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,7)\n",
    "# plt.imshow(train_x[6].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,8)\n",
    "# plt.imshow(train_x[7].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# plt.subplot(3,3,9)\n",
    "# plt.imshow(train_x[8].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "\n",
    "# # plt.subplot(1,2,2)\n",
    "# # plt.imshow(ds.train.images[3].reshape((28,28)), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "# # print train_y[2]\n",
    "# # print ds.train.labels[3]\n",
    "\n",
    "# print type(ds.train.labels[0])\n",
    "# print train_y[:10]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
